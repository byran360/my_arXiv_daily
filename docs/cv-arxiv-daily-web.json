{"Robot & Agent": {"2508.05634": "|**2025-08-07**|**Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling**|Jianpeng Yao et.al.|[2508.05634](https://arxiv.org/abs/2508.05634)|**[link](https://github.com/wonderNefelibata/Awesome-LRM-Safety)**|\n", "2508.05614": "|**2025-08-07**|**OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks**|Zixuan Wang et.al.|[2508.05614](https://arxiv.org/abs/2508.05614)|**[link](https://huggingface.co/datasets/wangzx1210/OmniEAR)**|\n", "2508.05543": "|**2025-08-07**|**CleanUpBench: Embodied Sweeping and Grasping Benchmark**|Wenbo Li et.al.|[2508.05543](https://arxiv.org/abs/2508.05543)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05294": "|**2025-08-07**|**Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction**|Sahar Salimpour et.al.|[2508.05294](https://arxiv.org/abs/2508.05294)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.03645": "|**2025-08-05**|**DiWA: Diffusion Policy Adaptation with World Models**|Akshay L Chandra et.al.|[2508.03645](https://arxiv.org/abs/2508.03645)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2508.03068": "|**2025-08-07**|**Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching**|Sirui Chen et.al.|[2508.03068](https://arxiv.org/abs/2508.03068)|**[link](https://github.com/YanjieZe/awesome-humanoid-robot-learning)**|\n", "2508.02870": "|**2025-08-04**|**Learning User Interaction Forces using Vision for a Soft Finger Exosuit**|Mohamed Irfan Refai et.al.|[2508.02870](https://arxiv.org/abs/2508.02870)|null|\n", "2508.02629": "|**2025-08-06**|**HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents**|Yibin Liu et.al.|[2508.02629](https://arxiv.org/abs/2508.02629)|**[link](https://github.com/jyyang621/DailyArXiv)**|\n", "2508.02376": "|**2025-08-04**|**Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction**|Matus Krajcovic et.al.|[2508.02376](https://arxiv.org/abs/2508.02376)|null|\n", "2508.02146": "|**2025-08-04**|**ScrewSplat: An End-to-End Method for Articulated Object Recognition**|Seungyeon Kim et.al.|[2508.02146](https://arxiv.org/abs/2508.02146)|**[link](https://github.com/seungyeon-k/ScrewSplat-public)**|\n", "2508.02062": "|**2025-08-04**|**RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models**|Kaustubh Sridhar et.al.|[2508.02062](https://arxiv.org/abs/2508.02062)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.01808": "|**2025-08-03**|**Learning to Perform Low-Contact Autonomous Nasotracheal Intubation by Recurrent Action-Confidence Chunking with Transformer**|Yu Tian et.al.|[2508.01808](https://arxiv.org/abs/2508.01808)|null|\n", "2508.01766": "|**2025-08-05**|**VPN: Visual Prompt Navigation**|Shuo Feng et.al.|[2508.01766](https://arxiv.org/abs/2508.01766)|**[link](https://github.com/farlit/VPN)**|\n", "2508.01723": "|**2025-08-03**|**OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping**|Danyang Li et.al.|[2508.01723](https://arxiv.org/abs/2508.01723)|null|\n", "2508.01651": "|**2025-08-03**|**DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding**|Hanqing Wang et.al.|[2508.01651](https://arxiv.org/abs/2508.01651)|null|\n", "2508.01600": "|**2025-08-03**|**CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation**|Sung-Wook Lee et.al.|[2508.01600](https://arxiv.org/abs/2508.01600)|null|\n", "2508.01131": "|**2025-08-02**|**COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning**|Sateesh Kumar et.al.|[2508.01131](https://arxiv.org/abs/2508.01131)|null|\n", "2508.00795": "|**2025-08-01**|**Video Generators are Robot Policies**|Junbang Liang et.al.|[2508.00795](https://arxiv.org/abs/2508.00795)|null|\n", "2508.00500": "|**2025-08-01**|**Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking**|Haoyu Wang et.al.|[2508.00500](https://arxiv.org/abs/2508.00500)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|\n", "2508.00400": "|**2025-08-01**|**Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents**|Janika Deborah Gajo et.al.|[2508.00400](https://arxiv.org/abs/2508.00400)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|\n", "2508.00354": "|**2025-08-01**|**Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging**|Tianshuang Qiu et.al.|[2508.00354](https://arxiv.org/abs/2508.00354)|**[link](https://github.com/liliu-avril/Awesome-Segment-Anything)**|\n", "2507.23772": "|**2025-07-31**|**SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting**|Di Li et.al.|[2507.23772](https://arxiv.org/abs/2507.23772)|**[link](https://github.com/longxiang-ai/awesome-gaussians)**|\n", "2507.23523": "|**2025-08-01**|**H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation**|Hongzhe Bi et.al.|[2507.23523](https://arxiv.org/abs/2507.23523)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.23391": "|**2025-07-31**|**Policy Learning from Large Vision-Language Model Feedback without Reward Modeling**|Tung M. Luu et.al.|[2507.23391](https://arxiv.org/abs/2507.23391)|**[link](https://github.com/tunglm2203/plare)**|\n", "2507.22028": "|**2025-07-29**|**From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning**|Honglin He et.al.|[2507.22028](https://arxiv.org/abs/2507.22028)|**[link](https://github.com/ai4ce/CityWalker)**|\n", "2507.21981": "|**2025-07-29**|**DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments**|Yufei Jia et.al.|[2507.21981](https://arxiv.org/abs/2507.21981)|**[link](https://github.com/TATP-233/DISCOVERSE)**|\n", "2507.21796": "|**2025-07-29**|**MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects**|Yuying Zhang et.al.|[2507.21796](https://arxiv.org/abs/2507.21796)|null|\n", "2507.20445": "|**2025-08-03**|**Learning Physical Interaction Skills from Human Demonstrations**|Tianyu Li et.al.|[2507.20445](https://arxiv.org/abs/2507.20445)|null|\n", "2507.19854": "|**2025-07-26**|**Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models**|Anjali R. Menon et.al.|[2507.19854](https://arxiv.org/abs/2507.19854)|null|\n", "2507.18623": "|**2025-07-26**|**Moving Out: Physically-grounded Human-AI Collaboration**|Xuhui Kang et.al.|[2507.18623](https://arxiv.org/abs/2507.18623)|null|\n", "2507.18342": "|**2025-07-24**|**EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs**|Yuping He et.al.|[2507.18342](https://arxiv.org/abs/2507.18342)|null|\n", "2507.17462": "|**2025-07-23**|**ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents**|Chang Nie et.al.|[2507.17462](https://arxiv.org/abs/2507.17462)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|\n", "2507.17294": "|**2025-07-29**|**VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback**|Jianxin Bi et.al.|[2507.17294](https://arxiv.org/abs/2507.17294)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.17275": "|**2025-07-25**|**Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning**|Po-Yen Wu et.al.|[2507.17275](https://arxiv.org/abs/2507.17275)|null|\n", "2507.17141": "|**2025-07-23**|**Towards Human-level Intelligence via Human-like Whole-Body Manipulation**|Guang Gao et.al.|[2507.17141](https://arxiv.org/abs/2507.17141)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.16124": "|**2025-07-22**|**Benchmarking LLM Privacy Recognition for Social Robot Decision Making**|Dakota Sullivan et.al.|[2507.16124](https://arxiv.org/abs/2507.16124)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|\n", "2507.15833": "|**2025-07-21**|**Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers**|Ian Chuang et.al.|[2507.15833](https://arxiv.org/abs/2507.15833)|null|\n", "2507.15493": "|**2025-07-22**|**GR-3 Technical Report**|Chilam Cheang et.al.|[2507.15493](https://arxiv.org/abs/2507.15493)|null|\n", "2507.15428": "|**2025-07-21**|**EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent**|Jiaao Li et.al.|[2507.15428](https://arxiv.org/abs/2507.15428)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|\n", "2507.15155": "|**2025-07-20**|**Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions**|Majid Roshanfar et.al.|[2507.15155](https://arxiv.org/abs/2507.15155)|null|\n", "2507.14904": "|**2025-07-20**|**TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP**|Fan Li et.al.|[2507.14904](https://arxiv.org/abs/2507.14904)|**[link](https://github.com/Lyz103/LLM-Agent-Paper-daily)**|\n", "2507.13340": "|**2025-07-17**|**Latent Policy Steering with Embodiment-Agnostic Pretrained World Models**|Yiqi Wang et.al.|[2507.13340](https://arxiv.org/abs/2507.13340)|**[link](https://github.com/LMD0311/Awesome-World-Model)**|\n", "2507.12731": "|**2025-07-17**|**Learning to Predict Mobile Robot Stability in Off-Road Environments**|Nathaniel Rose et.al.|[2507.12731](https://arxiv.org/abs/2507.12731)|null|\n", "2507.12440": "|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al.|[2507.12440](https://arxiv.org/abs/2507.12440)|null|\n", "2507.10961": "|**2025-07-15**|**EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks**|Joohwan Seo et.al.|[2507.10961](https://arxiv.org/abs/2507.10961)|null|\n", "2507.10722": "|**2025-07-14**|**Bridging Brains and Machines: A Unified Frontier in Neuroscience, Artificial Intelligence, and Neuromorphic Systems**|Sohan Shankar et.al.|[2507.10722](https://arxiv.org/abs/2507.10722)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|\n", "2507.10543": "|**2025-07-29**|**MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation**|Juyi Sheng et.al.|[2507.10543](https://arxiv.org/abs/2507.10543)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|\n", "2507.10158": "|**2025-07-16**|**MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping**|Obaidullah Zaland et.al.|[2507.10158](https://arxiv.org/abs/2507.10158)|null|\n", "2507.09540": "|**2025-07-13**|**Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling**|Ali Safa et.al.|[2507.09540](https://arxiv.org/abs/2507.09540)|**[link](https://github.com/SpikingChen/SNN-Daily-Arxiv)**|\n", "2507.09180": "|**2025-07-15**|**Learning and Transferring Better with Depth Information in Visual Reinforcement Learning**|Zichun Xu et.al.|[2507.09180](https://arxiv.org/abs/2507.09180)|null|\n", "2507.09117": "|**2025-07-12**|**Towards Human-level Dexterity via Robot Learning**|Gagan Khandate et.al.|[2507.09117](https://arxiv.org/abs/2507.09117)|null|\n", "2507.09061": "|**2025-07-26**|**Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction**|Thomas T. Zhang et.al.|[2507.09061](https://arxiv.org/abs/2507.09061)|null|\n", "2507.08726": "|**2025-07-11**|**Learning human-to-robot handovers through 3D scene reconstruction**|Yuekun Wu et.al.|[2507.08726](https://arxiv.org/abs/2507.08726)|**[link](https://github.com/Lee-JaeWon/2025-Arxiv-Paper-List-Gaussian-Splatting)**|\n", "2507.08303": "|**2025-07-11**|**Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots**|Yang Zhang et.al.|[2507.08303](https://arxiv.org/abs/2507.08303)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.07299": "|**2025-07-09**|**LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation**|Sonia Raychaudhuri et.al.|[2507.07299](https://arxiv.org/abs/2507.07299)|**[link](https://github.com/Lyz103/LLM-Agent-Paper-daily)**|\n", "2507.06219": "|**2025-07-08**|**Is Diversity All You Need for Scalable Robotic Manipulation?**|Modi Shi et.al.|[2507.06219](https://arxiv.org/abs/2507.06219)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.06172": "|**2025-07-08**|**Learning Agile Tensile Perching for Aerial Robots from Demonstrations**|Kangle Yuan et.al.|[2507.06172](https://arxiv.org/abs/2507.06172)|**[link](https://github.com/AerialRoboticsGroup/agile-tethered-perching)**|\n", "2507.06016": "|**2025-07-08**|**Conditional Multi-Stage Failure Recovery for Embodied Agents**|Youmna Farag et.al.|[2507.06016](https://arxiv.org/abs/2507.06016)|**[link](https://github.com/AGI-Edgerunners/LLM-Agents-Papers)**|\n", "2507.05695": "|**2025-07-08**|**Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning**|Xiatao Sun et.al.|[2507.05695](https://arxiv.org/abs/2507.05695)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.05198": "|**2025-07-07**|**EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling**|Boyuan Wang et.al.|[2507.05198](https://arxiv.org/abs/2507.05198)|**[link](https://github.com/leofan90/Awesome-World-Models)**|\n"}, "Robotic Manipulation": {"2508.05635": "|**2025-08-07**|**Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation**|Yue Liao et.al.|[2508.05635](https://arxiv.org/abs/2508.05635)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2508.05625": "|**2025-08-07**|**How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations**|Brandon Jaipersaud et.al.|[2508.05625](https://arxiv.org/abs/2508.05625)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05621": "|**2025-08-07**|**Back to Bits: Extending Shannon's communication performance framework to computing**|Max Hawkins et.al.|[2508.05621](https://arxiv.org/abs/2508.05621)|null|\n", "2508.05619": "|**2025-08-07**|**The Missing Reward: Active Inference in the Era of Experience**|Bo Wen et.al.|[2508.05619](https://arxiv.org/abs/2508.05619)|null|\n", "2508.05608": "|**2025-08-07**|**Ultra-Large-Scale Compilation and Manipulation of Quantum Circuits with Pandora**|Ioana Moflic et.al.|[2508.05608](https://arxiv.org/abs/2508.05608)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05584": "|**2025-08-07**|**Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator**|Van Cuong Pham et.al.|[2508.05584](https://arxiv.org/abs/2508.05584)|**[link](https://github.com/zezhishao/DailyArXiv)**|\n", "2508.05543": "|**2025-08-07**|**CleanUpBench: Embodied Sweeping and Grasping Benchmark**|Wenbo Li et.al.|[2508.05543](https://arxiv.org/abs/2508.05543)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05535": "|**2025-08-07**|**Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation**|Albert Yu et.al.|[2508.05535](https://arxiv.org/abs/2508.05535)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05526": "|**2025-08-07**|**When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework**|Haoyu Liu et.al.|[2508.05526](https://arxiv.org/abs/2508.05526)|**[link](https://github.com/zezhishao/DailyArXiv)**|\n", "2508.05469": "|**2025-08-07**|**Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes**|Zachary Robertson et.al.|[2508.05469](https://arxiv.org/abs/2508.05469)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|\n", "2508.05415": "|**2025-08-07**|**Do Robots Really Need Anthropomorphic Hands?**|Alexander Fabisch et.al.|[2508.05415](https://arxiv.org/abs/2508.05415)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05409": "|**2025-08-07**|**From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization**|Farah Wahida et.al.|[2508.05409](https://arxiv.org/abs/2508.05409)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|\n", "2508.05396": "|**2025-08-07**|**Real-Time Iteration Scheme for Diffusion Policy**|Yufei Duan et.al.|[2508.05396](https://arxiv.org/abs/2508.05396)|**[link](https://huggingface.co/models/duandaxia/rti-dp-scale)**|\n", "2508.05356": "|**2025-08-07**|**Symmetry Packaging I: Irreducible Representation Blocks, Superselection, and Packaged Entanglement in Quantum Field Theory**|Rongchao Ma et.al.|[2508.05356](https://arxiv.org/abs/2508.05356)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05342": "|**2025-08-07**|**Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control**|Shunlei Li et.al.|[2508.05342](https://arxiv.org/abs/2508.05342)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.05310": "|**2025-08-07**|**ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning**|Jelle Luijkx et.al.|[2508.05310](https://arxiv.org/abs/2508.05310)|null|\n", "2508.05186": "|**2025-08-07**|**Learning to See and Act: Task-Aware View Planning for Robotic Manipulation**|Yongjie Bai et.al.|[2508.05186](https://arxiv.org/abs/2508.05186)|null|\n", "2508.05153": "|**2025-08-07**|**FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction**|Mohammed Daba et.al.|[2508.05153](https://arxiv.org/abs/2508.05153)|null|\n", "2508.05112": "|**2025-08-07**|**Metacognition and self-regulated learning in manipulative robotic problem-solving task**|Margarida Romero et.al.|[2508.05112](https://arxiv.org/abs/2508.05112)|null|\n", "2508.05076": "|**2025-08-07**|**MetaDiT: Enabling Fine-grained Constraints in High-degree-of Freedom Metasurface Design**|Hao Li et.al.|[2508.05076](https://arxiv.org/abs/2508.05076)|null|\n", "2508.05040": "|**2025-08-07**|**A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System**|Boyang Zhang et.al.|[2508.05040](https://arxiv.org/abs/2508.05040)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05014": "|**2025-08-07**|**Wave Computing based on Dynamical Networks: Applications in Optimization Problems**|Yunwen Liu et.al.|[2508.05014](https://arxiv.org/abs/2508.05014)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.04931": "|**2025-08-06**|**INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM**|Jin Wang et.al.|[2508.04931](https://arxiv.org/abs/2508.04931)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.04892": "|**2025-08-06**|**Fault-Tolerant Universal Quantum Computing in the Presence of Anisotropic Noise**|Yang-Yang Xie et.al.|[2508.04892](https://arxiv.org/abs/2508.04892)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.04859": "|**2025-08-06**|**An Implementation of a Visual Stepper in the GRASP Programming System**|Panicz Maciej Godek et.al.|[2508.04859](https://arxiv.org/abs/2508.04859)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.04828": "|**2025-08-06**|**Modelling the emergence of open-ended technological evolution**|James Winters et.al.|[2508.04828](https://arxiv.org/abs/2508.04828)|**[link](https://github.com/j-winters/bitw0r1d)**|\n", "2508.04821": "|**2025-08-06**|**At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp**|Yang Liu et.al.|[2508.04821](https://arxiv.org/abs/2508.04821)|null|\n", "2508.04465": "|**2025-08-06**|**Tight-binding photonics**|Jing Li et.al.|[2508.04465](https://arxiv.org/abs/2508.04465)|null|\n", "2508.04282": "|**2025-08-06**|**Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling**|Yongyi Wang et.al.|[2508.04282](https://arxiv.org/abs/2508.04282)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|\n", "2508.04281": "|**2025-08-06**|**Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy**|Jairo Gudi\u00f1o-Rosero et.al.|[2508.04281](https://arxiv.org/abs/2508.04281)|**[link](https://github.com/shaokangW/LLM-wisdom)**|\n", "2508.04276": "|**2025-08-06**|**A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models**|Jiayi Wen et.al.|[2508.04276](https://arxiv.org/abs/2508.04276)|**[link](https://github.com/shaokangW/LLM-wisdom)**|\n", "2508.04196": "|**2025-08-06**|**Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models**|Siddhant Panpatil et.al.|[2508.04196](https://arxiv.org/abs/2508.04196)|**[link](https://github.com/shaokangW/LLM-wisdom)**|\n", "2508.04188": "|**2025-08-06**|**$\\boldsymbol{d}$-vector precession induced pumping in topological $p$-wave superconductors**|Jun-Jie Fu et.al.|[2508.04188](https://arxiv.org/abs/2508.04188)|null|\n", "2508.04043": "|**2025-08-06**|**VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning**|Yuheng Ji et.al.|[2508.04043](https://arxiv.org/abs/2508.04043)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.04024": "|**2025-08-06**|**Identity Theft in AI Conference Peer Review**|Nihar B. Shah et.al.|[2508.04024](https://arxiv.org/abs/2508.04024)|null|\n", "2508.04009": "|**2025-08-06**|**Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)**|Vu Ngoc Son et.al.|[2508.04009](https://arxiv.org/abs/2508.04009)|null|\n", "2508.03946": "|**2025-08-05**|**Sub-5-fs compression and synchronization of relativistic electron bunches enabled by a high-gradient $\u03b1$-magnet and low-jitter photoinjector**|Yining Yang et.al.|[2508.03946](https://arxiv.org/abs/2508.03946)|null|\n", "2508.03944": "|**2025-08-05**|**Constraint-Preserving Data Generation for Visuomotor Policy Learning**|Kevin Lin et.al.|[2508.03944](https://arxiv.org/abs/2508.03944)|null|\n", "2508.03923": "|**2025-08-08**|**CoAct-1: Computer-using Agents with Coding as Actions**|Linxin Song et.al.|[2508.03923](https://arxiv.org/abs/2508.03923)|**[link](https://github.com/kyegomez/awesome-multi-agent-papers)**|\n", "2508.03894": "|**2025-08-05**|**Particle manipulation by hydrodynamic effects in vortical Stokes flow**|Xuchen Liu et.al.|[2508.03894](https://arxiv.org/abs/2508.03894)|null|\n", "2508.03692": "|**2025-08-05**|**LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences**|Ao Liang et.al.|[2508.03692](https://arxiv.org/abs/2508.03692)|**[link](https://github.com/leofan90/Awesome-World-Models)**|\n", "2508.03793": "|**2025-08-05**|**AttnTrace: Attention-based Context Traceback for Long-Context LLMs**|Yanting Wang et.al.|[2508.03793](https://arxiv.org/abs/2508.03793)|**[link](https://huggingface.co/spaces/SecureLLMSys/AttnTrace)**|\n", "2508.03645": "|**2025-08-05**|**DiWA: Diffusion Policy Adaptation with World Models**|Akshay L Chandra et.al.|[2508.03645](https://arxiv.org/abs/2508.03645)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2508.03599": "|**2025-08-05**|**OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War**|Johannes Niu et.al.|[2508.03599](https://arxiv.org/abs/2508.03599)|null|\n", "2508.03543": "|**2025-08-06**|**EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable Text-to-Speech via Activation Steering**|Tianxin Xie et.al.|[2508.03543](https://arxiv.org/abs/2508.03543)|**[link](https://github.com/iszhanjiawei/flow_matching_arxiv_daily)**|\n", "2508.03526": "|**2025-08-05**|**CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation**|Kun Song et.al.|[2508.03526](https://arxiv.org/abs/2508.03526)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2508.03515": "|**2025-08-06**|**Oxide Interface-Based Polymorphic Electronic Devices for Neuromorphic Computing**|Soumen Pradhan et.al.|[2508.03515](https://arxiv.org/abs/2508.03515)|null|\n", "2508.03512": "|**2025-08-05**|**Homogenization rates of beam lattices to micropolar continua**|Eric T. Chung et.al.|[2508.03512](https://arxiv.org/abs/2508.03512)|null|\n", "2508.03480": "|**2025-08-05**|**VideoGuard: Protecting Video Content from Unauthorized Editing**|Junjie Cao et.al.|[2508.03480](https://arxiv.org/abs/2508.03480)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|\n", "2508.02823": "|**2025-08-05**|**NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification**|Wenshuo Zhang et.al.|[2508.02823](https://arxiv.org/abs/2508.02823)|null|\n", "2508.03374": "|**2025-08-05**|**GRASPing Anatomy to Improve Pathology Segmentation**|Keyi Li et.al.|[2508.03374](https://arxiv.org/abs/2508.03374)|null|\n", "2508.03365": "|**2025-08-05**|**When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs**|Bodam Kim et.al.|[2508.03365](https://arxiv.org/abs/2508.03365)|null|\n", "2508.03364": "|**2025-08-05**|**Engineering subgap states in superconductors by altermagnetism**|Bo Lu et.al.|[2508.03364](https://arxiv.org/abs/2508.03364)|null|\n", "2508.03339": "|**2025-08-05**|**UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands**|Haoran Lin et.al.|[2508.03339](https://arxiv.org/abs/2508.03339)|**[link](https://github.com/haochen611/UFG)**|\n", "2508.03221": "|**2025-08-05**|**BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models**|Yu Pan et.al.|[2508.03221](https://arxiv.org/abs/2508.03221)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|\n", "2508.03218": "|**2025-08-05**|**ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow**|Shanshan Guo et.al.|[2508.03218](https://arxiv.org/abs/2508.03218)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2508.03120": "|**2025-08-05**|**Can Large Language Models Identify Materials from Radar Signals?**|Jiangyou Zhu et.al.|[2508.03120](https://arxiv.org/abs/2508.03120)|null|\n", "2508.03110": "|**2025-08-05**|**Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation**|Zizhong Li et.al.|[2508.03110](https://arxiv.org/abs/2508.03110)|null|\n", "2508.03099": "|**2025-08-05**|**Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping**|Sang Min Kim et.al.|[2508.03099](https://arxiv.org/abs/2508.03099)|null|\n", "2508.03054": "|**2025-08-05**|**Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning**|Rui Pu et.al.|[2508.03054](https://arxiv.org/abs/2508.03054)|null|\n"}, "Vision Language Action Model": {"2508.05342": "|**2025-08-07**|**Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control**|Shunlei Li et.al.|[2508.05342](https://arxiv.org/abs/2508.05342)|null|\n", "2508.05294": "|**2025-08-07**|**Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction**|Sahar Salimpour et.al.|[2508.05294](https://arxiv.org/abs/2508.05294)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.02219": "|**2025-08-04**|**CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning**|Dongchi Huang et.al.|[2508.02219](https://arxiv.org/abs/2508.02219)|**[link](https://github.com/XiaoWei-i/Awesome-VLA-RL)**|\n", "2508.02062": "|**2025-08-04**|**RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models**|Kaustubh Sridhar et.al.|[2508.02062](https://arxiv.org/abs/2508.02062)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.00097": "|**2025-07-31**|**XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation**|Zhigen Zhao et.al.|[2508.00097](https://arxiv.org/abs/2508.00097)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2507.23682": "|**2025-07-31**|**villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models**|Xiaoyu Chen et.al.|[2507.23682](https://arxiv.org/abs/2507.23682)|**[link](https://huggingface.co/models/microsoft/villa-x)**|\n", "2507.22424": "|**2025-07-30**|**Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance**|Songsheng Wang et.al.|[2507.22424](https://arxiv.org/abs/2507.22424)|**[link](https://github.com/hemingkx/SpeculativeDecodingPapers)**|\n", "2507.17383": "|**2025-07-23**|**Confidence Calibration in Vision-Language-Action Models**|Thomas P Zollo et.al.|[2507.17383](https://arxiv.org/abs/2507.17383)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.17294": "|**2025-07-29**|**VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback**|Jianxin Bi et.al.|[2507.17294](https://arxiv.org/abs/2507.17294)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.15597": "|**2025-07-21**|**Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos**|Hao Luo et.al.|[2507.15597](https://arxiv.org/abs/2507.15597)|**[link](https://huggingface.co/models/BeingBeyond/Being-H0)**|\n", "2507.14049": "|**2025-07-18**|**EdgeVLA: Efficient Vision-Language-Action Models**|Pawe\u0142 Budzianowski et.al.|[2507.14049](https://arxiv.org/abs/2507.14049)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.12440": "|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al.|[2507.12440](https://arxiv.org/abs/2507.12440)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.10672": "|**2025-07-14**|**Vision Language Action Models in Robotic Manipulation: A Systematic Review**|Muhayy Ud Din et.al.|[2507.10672](https://arxiv.org/abs/2507.10672)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.09160": "|**2025-07-12**|**Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization**|Jialei Huang et.al.|[2507.09160](https://arxiv.org/abs/2507.09160)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.06484": "|**2025-07-09**|**3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds**|Fan-Yun Sun et.al.|[2507.06484](https://arxiv.org/abs/2507.06484)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|\n", "2507.05227": "|**2025-07-07**|**NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving**|Qucheng Peng et.al.|[2507.05227](https://arxiv.org/abs/2507.05227)|**[link](https://github.com/Xuchen-Li/cv-arxiv-daily)**|\n", "2507.04447": "|**2025-07-17**|**DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge**|Wenyao Zhang et.al.|[2507.04447](https://arxiv.org/abs/2507.04447)|**[link](https://huggingface.co/models/WenyaoZhang/DreamVLA)**|\n", "2507.01925": "|**2025-07-02**|**A Survey on Vision-Language-Action Models: An Action Tokenization Perspective**|Yifan Zhong et.al.|[2507.01925](https://arxiv.org/abs/2507.01925)|**[link](https://github.com/TianxingChen/Embodied-AI-Guide)**|\n", "2507.01843": "|**2025-07-02**|**MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics**|Dmytro Kuzmenko et.al.|[2507.01843](https://arxiv.org/abs/2507.01843)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.01424": "|**2025-07-03**|**TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control**|Zhenyang Liu et.al.|[2507.01424](https://arxiv.org/abs/2507.01424)|**[link](https://github.com/Jiaaqiliu/Awesome-VLA-Robotics)**|\n", "2507.01016": "|**2025-07-01**|**VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers**|Yating Wang et.al.|[2507.01016](https://arxiv.org/abs/2507.01016)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.00416": "|**2025-07-01**|**Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding**|Tao Lin et.al.|[2507.00416](https://arxiv.org/abs/2507.00416)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2506.24044": "|**2025-06-30**|**A Survey on Vision-Language-Action Models for Autonomous Driving**|Sicong Jiang et.al.|[2506.24044](https://arxiv.org/abs/2506.24044)|**[link](https://github.com/52CV/CV-Surveys)**|\n", "2506.19850": "|**2025-06-24**|**Unified Vision-Language-Action Model**|Yuqi Wang et.al.|[2506.19850](https://arxiv.org/abs/2506.19850)|**[link](https://huggingface.co/models/Yuqi1997/UniVLA)**|\n", "2506.17811": "|**2025-07-07**|**RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models**|Jacky Kwok et.al.|[2506.17811](https://arxiv.org/abs/2506.17811)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2506.17639": "|**2025-06-21**|**RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models**|Yuxuan Chen et.al.|[2506.17639](https://arxiv.org/abs/2506.17639)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2506.17561": "|**2025-06-21**|**VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models**|Chongkai Gao et.al.|[2506.17561](https://arxiv.org/abs/2506.17561)|**[link](https://huggingface.co/models/Linslab/VLA-OS)**|\n", "2506.16211": "|**2025-06-19**|**ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models**|Puhao Li et.al.|[2506.16211](https://arxiv.org/abs/2506.16211)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2506.14317": "|**2025-06-19**|**ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes**|Zeyuan Chen et.al.|[2506.14317](https://arxiv.org/abs/2506.14317)|null|\n", "2506.13757": "|**2025-06-16**|**AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning**|Zewei Zhou et.al.|[2506.13757](https://arxiv.org/abs/2506.13757)|null|\n", "2506.13725": "|**2025-06-16**|**CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding**|Wenxuan Song et.al.|[2506.13725](https://arxiv.org/abs/2506.13725)|**[link](https://huggingface.co/models/chenpyyy/openvla-ac)**|\n", "2506.13456": "|**2025-06-16**|**Block-wise Adaptive Caching for Accelerating Diffusion Policy**|Kangye Ji et.al.|[2506.13456](https://arxiv.org/abs/2506.13456)|null|\n", "2506.13045": "|**2025-06-19**|**A Comprehensive Survey on Continual Learning in Generative Models**|Haiyang Guo et.al.|[2506.13045](https://arxiv.org/abs/2506.13045)|null|\n", "2506.10826": "|**2025-06-13**|**RationalVLA: A Rational Vision-Language-Action Model with Dual System**|Wenxuan Song et.al.|[2506.10826](https://arxiv.org/abs/2506.10826)|null|\n", "2506.10100": "|**2025-06-11**|**EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models**|Yantai Yang et.al.|[2506.10100](https://arxiv.org/abs/2506.10100)|**[link](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling)**|\n", "2506.09937": "|**2025-06-11**|**SAFE: Multitask Failure Detection for Vision-Language-Action Models**|Qiao Gu et.al.|[2506.09937](https://arxiv.org/abs/2506.09937)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2506.09930": "|**2025-06-11**|**From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models**|Irving Fang et.al.|[2506.09930](https://arxiv.org/abs/2506.09930)|**[link](https://huggingface.co/models/IPEC-COMMUNITY/spatialvla-4b-224-sft-bridge)**|\n", "2506.09172": "|**2025-06-17**|**An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models**|Pranav Guruprasad et.al.|[2506.09172](https://arxiv.org/abs/2506.09172)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2506.08440": "|**2025-06-11**|**TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization**|Zengjue Chen et.al.|[2506.08440](https://arxiv.org/abs/2506.08440)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2506.07530": "|**2025-06-09**|**BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation**|Hongyu Wang et.al.|[2506.07530](https://arxiv.org/abs/2506.07530)|**[link](https://huggingface.co/models/hongyuw/bitvla-bitsiglipL-224px-bf16)**|\n", "2506.07339": "|**2025-06-09**|**Real-Time Execution of Action Chunking Flow Policies**|Kevin Black et.al.|[2506.07339](https://arxiv.org/abs/2506.07339)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2506.06535": "|**2025-07-20**|**MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping**|Vineet Bhat et.al.|[2506.06535](https://arxiv.org/abs/2506.06535)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2506.03574": "|**2025-06-04**|**SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models**|Meng Li et.al.|[2506.03574](https://arxiv.org/abs/2506.03574)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2506.03350": "|**2025-06-03**|**Adversarial Attacks on Robotic Vision Language Action Models**|Eliot Krzysztof Jones et.al.|[2506.03350](https://arxiv.org/abs/2506.03350)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2506.01844": "|**2025-06-02**|**SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics**|Mustafa Shukor et.al.|[2506.01844](https://arxiv.org/abs/2506.01844)|**[link](https://huggingface.co/spaces/arpitg1304/lerobot_scripts_simplified)**|\n", "2506.01300": "|**2025-06-02**|**ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding**|Yiyang Zhou et.al.|[2506.01300](https://arxiv.org/abs/2506.01300)|null|\n", "2506.01196": "|**2025-06-01**|**OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation**|Ishika Singh et.al.|[2506.01196](https://arxiv.org/abs/2506.01196)|null|\n", "2506.00411": "|**2025-05-31**|**LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks**|Yi Yang et.al.|[2506.00411](https://arxiv.org/abs/2506.00411)|null|\n", "2505.23757": "|**2025-05-29**|**Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models**|Haohan Chi et.al.|[2505.23757](https://arxiv.org/abs/2505.23757)|null|\n", "2505.23705": "|**2025-05-29**|**Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better**|Danny Driess et.al.|[2505.23705](https://arxiv.org/abs/2505.23705)|null|\n", "2505.23450": "|**2025-06-11**|**Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents**|Zhejian Yang et.al.|[2505.23450](https://arxiv.org/abs/2505.23450)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2505.21906": "|**2025-05-29**|**ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge**|Zhongyi Zhou et.al.|[2505.21906](https://arxiv.org/abs/2505.21906)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2505.21432": "|**2025-07-08**|**Hume: Introducing System-2 Thinking in Visual-Language-Action Model**|Haoming Song et.al.|[2505.21432](https://arxiv.org/abs/2505.21432)|**[link](https://huggingface.co/models/Hume-vla/Hume-System2)**|\n", "2505.21200": "|**2025-05-27**|**Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models**|Xudong Tan et.al.|[2505.21200](https://arxiv.org/abs/2505.21200)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2505.21567": "|**2025-07-31**|**EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models**|Feng Jiang et.al.|[2505.21567](https://arxiv.org/abs/2505.21567)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2505.20503": "|**2025-05-26**|**Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review**|Matthew Lisondra et.al.|[2505.20503](https://arxiv.org/abs/2505.20503)|**[link](https://github.com/52CV/CV-Surveys)**|\n", "2505.17016": "|**2025-05-22**|**Interactive Post-Training for Vision-Language-Action Models**|Shuhan Tan et.al.|[2505.17016](https://arxiv.org/abs/2505.17016)|**[link](https://huggingface.co/models/tanshh97/RIPT_VLA)**|\n", "2505.16815": "|**2025-05-22**|**Perceptual Quality Assessment for Embodied AI**|Chunyi Li et.al.|[2505.16815](https://arxiv.org/abs/2505.16815)|**[link](https://github.com/chenin-wang/awesome_ai_paper)**|\n", "2505.16640": "|**2025-05-22**|**BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization**|Xueyang Zhou et.al.|[2505.16640](https://arxiv.org/abs/2505.16640)|**[link](https://github.com/GT-RIPL/Awesome-LLM-Robotics)**|\n", "2505.16278": "|**2025-05-22**|**DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving**|Zhenjie Yang et.al.|[2505.16278](https://arxiv.org/abs/2505.16278)|**[link](https://github.com/Thinklab-SJTU/Awesome-LLM4AD)**|\n"}, "Imitation Learning": {"2508.05310": "|**2025-08-07**|**ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning**|Jelle Luijkx et.al.|[2508.05310](https://arxiv.org/abs/2508.05310)|null|\n", "2508.05081": "|**2025-08-07**|**Cognitive Duality for Adaptive Web Agents**|Jiarun Liu et.al.|[2508.05081](https://arxiv.org/abs/2508.05081)|null|\n", "2508.05077": "|**2025-08-07**|**Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning**|Luai Abuelsamen et.al.|[2508.05077](https://arxiv.org/abs/2508.05077)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.03129": "|**2025-08-05**|**Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection**|Le Qiu et.al.|[2508.03129](https://arxiv.org/abs/2508.03129)|**[link](https://github.com/dingyue772/DailyArxiv)**|\n", "2508.03043": "|**2025-08-05**|**Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control**|Yi-Hsuan Hsiao et.al.|[2508.03043](https://arxiv.org/abs/2508.03043)|**[link](https://github.com/Ed1sonChen/DailyArxiv)**|\n", "2508.02617": "|**2025-08-04**|**Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An Imitation Learning Approach**|Peng Wei et.al.|[2508.02617](https://arxiv.org/abs/2508.02617)|null|\n", "2508.02219": "|**2025-08-04**|**CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning**|Dongchi Huang et.al.|[2508.02219](https://arxiv.org/abs/2508.02219)|**[link](https://github.com/XiaoWei-i/Awesome-VLA-RL)**|\n", "2508.02062": "|**2025-08-04**|**RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models**|Kaustubh Sridhar et.al.|[2508.02062](https://arxiv.org/abs/2508.02062)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.01600": "|**2025-08-03**|**CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation**|Sung-Wook Lee et.al.|[2508.01600](https://arxiv.org/abs/2508.01600)|**[link](https://github.com/Ed1sonChen/DailyArxiv)**|\n", "2508.01442": "|**2025-08-02**|**Physically-based Lighting Augmentation for Robotic Manipulation**|Shutong Jin et.al.|[2508.01442](https://arxiv.org/abs/2508.01442)|null|\n", "2508.01167": "|**2025-08-02**|**T2S: Tokenized Skill Scaling for Lifelong Imitation Learning**|Hongquan Zhang et.al.|[2508.01167](https://arxiv.org/abs/2508.01167)|null|\n", "2508.01131": "|**2025-08-02**|**COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning**|Sateesh Kumar et.al.|[2508.01131](https://arxiv.org/abs/2508.01131)|null|\n", "2508.01060": "|**2025-08-01**|**Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation**|Ibrahim Althamary et.al.|[2508.01060](https://arxiv.org/abs/2508.01060)|null|\n", "2508.00795": "|**2025-08-01**|**Video Generators are Robot Policies**|Junbang Liang et.al.|[2508.00795](https://arxiv.org/abs/2508.00795)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2508.00697": "|**2025-08-01**|**On-Device Diffusion Transformer Policy for Efficient Robot Manipulation**|Yiming Wu et.al.|[2508.00697](https://arxiv.org/abs/2508.00697)|**[link](https://github.com/Songwxuan/Embodied-AI-Paper-TopConf)**|\n", "2508.00491": "|**2025-08-01**|**HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning**|Carlo Alessi et.al.|[2508.00491](https://arxiv.org/abs/2508.00491)|**[link](https://github.com/Ed1sonChen/DailyArxiv)**|\n", "2508.00261": "|**2025-08-01**|**Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning**|Saichao Liu et.al.|[2508.00261](https://arxiv.org/abs/2508.00261)|**[link](https://github.com/zezhishao/DailyArXiv)**|\n", "2507.23523": "|**2025-08-01**|**H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation**|Hongzhe Bi et.al.|[2507.23523](https://arxiv.org/abs/2507.23523)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.23053": "|**2025-07-30**|**In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion**|Yuanhao Chen et.al.|[2507.23053](https://arxiv.org/abs/2507.23053)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2507.22380": "|**2025-07-30**|**Improving Generalization Ability of Robotic Imitation Learning by Resolving Causal Confusion in Observations**|Yifei Chen et.al.|[2507.22380](https://arxiv.org/abs/2507.22380)|null|\n", "2507.22219": "|**2025-07-29**|**RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation**|Dongyub Jude Lee et.al.|[2507.22219](https://arxiv.org/abs/2507.22219)|null|\n", "2507.21981": "|**2025-07-29**|**DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments**|Yufei Jia et.al.|[2507.21981](https://arxiv.org/abs/2507.21981)|null|\n", "2507.21796": "|**2025-07-29**|**MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects**|Yuying Zhang et.al.|[2507.21796](https://arxiv.org/abs/2507.21796)|null|\n", "2507.21533": "|**2025-07-29**|**Model Predictive Adversarial Imitation Learning for Planning from Observation**|Tyler Han et.al.|[2507.21533](https://arxiv.org/abs/2507.21533)|null|\n", "2507.21452": "|**2025-07-29**|**Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training**|Sodtavilan Odonchimed et.al.|[2507.21452](https://arxiv.org/abs/2507.21452)|**[link](https://github.com/Ed1sonChen/DailyArxiv)**|\n", "2507.20622": "|**2025-07-28**|**FMimic: Foundation Models are Fine-grained Action Learners from Human Videos**|Guangyan Chen et.al.|[2507.20622](https://arxiv.org/abs/2507.20622)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2507.19854": "|**2025-07-26**|**Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models**|Anjali R. Menon et.al.|[2507.19854](https://arxiv.org/abs/2507.19854)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|\n", "2507.19817": "|**2025-07-26**|**Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation**|Ziyin Xiong et.al.|[2507.19817](https://arxiv.org/abs/2507.19817)|**[link](https://github.com/ziyin-xiong/Ag2x2)**|\n", "2507.19647": "|**2025-07-25**|**GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning**|Amin Banayeeanzade et.al.|[2507.19647](https://arxiv.org/abs/2507.19647)|null|\n", "2507.18884": "|**2025-07-25**|**MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service**|Ming Gong et.al.|[2507.18884](https://arxiv.org/abs/2507.18884)|null|\n", "2507.18436": "|**2025-07-24**|**Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning**|David Blanco-Mulero et.al.|[2507.18436](https://arxiv.org/abs/2507.18436)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2507.17462": "|**2025-07-23**|**ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents**|Chang Nie et.al.|[2507.17462](https://arxiv.org/abs/2507.17462)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|\n", "2507.17418": "|**2025-07-23**|**Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning**|Joobin Jin et.al.|[2507.17418](https://arxiv.org/abs/2507.17418)|**[link](https://github.com/zezhishao/DailyArXiv)**|\n", "2507.17309": "|**2025-07-23**|**Confounded Causal Imitation Learning with Instrumental Variables**|Yan Zeng et.al.|[2507.17309](https://arxiv.org/abs/2507.17309)|null|\n", "2507.17056": "|**2025-07-22**|**Pragmatic Policy Development via Interpretable Behavior Cloning**|Anton Matsson et.al.|[2507.17056](https://arxiv.org/abs/2507.17056)|null|\n", "2507.15493": "|**2025-07-22**|**GR-3 Technical Report**|Chilam Cheang et.al.|[2507.15493](https://arxiv.org/abs/2507.15493)|null|\n", "2507.15073": "|**2025-07-20**|**Reinforcement Learning for Flow-Matching Policies**|Samuel Pfrommer et.al.|[2507.15073](https://arxiv.org/abs/2507.15073)|null|\n", "2507.14995": "|**2025-07-20**|**LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading**|Chengwei Lou et.al.|[2507.14995](https://arxiv.org/abs/2507.14995)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|\n", "2507.16842": "|**2025-07-19**|**Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning**|Yinan Meng et.al.|[2507.16842](https://arxiv.org/abs/2507.16842)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|\n", "2507.13602": "|**2025-07-18**|**Improving Low-Cost Teleoperation: Augmenting GELLO with Force**|Shivakanth Sujit et.al.|[2507.13602](https://arxiv.org/abs/2507.13602)|null|\n", "2507.13340": "|**2025-07-17**|**Latent Policy Steering with Embodiment-Agnostic Pretrained World Models**|Yiqi Wang et.al.|[2507.13340](https://arxiv.org/abs/2507.13340)|**[link](https://github.com/LMD0311/Awesome-World-Model)**|\n", "2507.13332": "|**2025-07-17**|**The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner**|Zhouqi Hua et.al.|[2507.13332](https://arxiv.org/abs/2507.13332)|**[link](https://github.com/gabrielchua/daily-ai-papers)**|\n", "2507.13088": "|**2025-07-17**|**ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning**|Rahel Rickenbach et.al.|[2507.13088](https://arxiv.org/abs/2507.13088)|null|\n", "2507.12856": "|**2025-07-17**|**Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)**|Chongli Qin et.al.|[2507.12856](https://arxiv.org/abs/2507.12856)|**[link](https://huggingface.co/models/ChongliQin/iw-SFT-32B)**|\n", "2507.12440": "|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al.|[2507.12440](https://arxiv.org/abs/2507.12440)|null|\n", "2507.10899": "|**2025-07-15**|**Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning**|Wang Zhicheng et.al.|[2507.10899](https://arxiv.org/abs/2507.10899)|null|\n", "2507.10284": "|**2025-07-14**|**Prompt Informed Reinforcement Learning for Visual Coverage Path Planning**|Venkat Margapuri et.al.|[2507.10284](https://arxiv.org/abs/2507.10284)|null|\n", "2507.10174": "|**2025-07-14**|**Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?**|Yumi Omori et.al.|[2507.10174](https://arxiv.org/abs/2507.10174)|**[link](https://github.com/Lyz103/LLM-Agent-Paper-daily)**|\n", "2507.10628": "|**2025-07-16**|**GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning**|Ziru Liu et.al.|[2507.10628](https://arxiv.org/abs/2507.10628)|**[link](https://github.com/hkgc-1/GHPO)**|\n", "2507.09537": "|**2025-07-13**|**Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles**|Yangang Ren et.al.|[2507.09537](https://arxiv.org/abs/2507.09537)|**[link](https://github.com/Lyz103/LLM-Agent-Paper-daily)**|\n", "2507.09305": "|**2025-07-24**|**DAA*: Deep Angular A Star for Image-based Path Planning**|Zhiwei Xu et.al.|[2507.09305](https://arxiv.org/abs/2507.09305)|**[link](https://github.com/52CV/ICCV-2025-Papers)**|\n", "2507.09117": "|**2025-07-12**|**Towards Human-level Dexterity via Robot Learning**|Gagan Khandate et.al.|[2507.09117](https://arxiv.org/abs/2507.09117)|null|\n", "2507.09061": "|**2025-07-26**|**Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction**|Thomas T. Zhang et.al.|[2507.09061](https://arxiv.org/abs/2507.09061)|null|\n", "2507.09041": "|**2025-07-11**|**Behavioral Exploration: Learning to Explore via In-Context Adaptation**|Andrew Wagenmaker et.al.|[2507.09041](https://arxiv.org/abs/2507.09041)|null|\n", "2507.08112": "|**2025-07-10**|**Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion**|Lamiaa H. Zain et.al.|[2507.08112](https://arxiv.org/abs/2507.08112)|null|\n", "2507.07986": "|**2025-07-15**|**EXPO: Stable Reinforcement Learning with Expressive Policies**|Perry Dong et.al.|[2507.07986](https://arxiv.org/abs/2507.07986)|null|\n", "2507.07969": "|**2025-07-15**|**Reinforcement Learning with Action Chunking**|Qiyang Li et.al.|[2507.07969](https://arxiv.org/abs/2507.07969)|null|\n", "2507.06780": "|**2025-07-09**|**Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm**|George Papadopoulos et.al.|[2507.06780](https://arxiv.org/abs/2507.06780)|null|\n", "2507.06710": "|**2025-07-13**|**Spatial-Temporal Aware Visuomotor Diffusion Policy Learning**|Zhenyang Liu et.al.|[2507.06710](https://arxiv.org/abs/2507.06710)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|\n", "2507.06701": "|**2025-07-09**|**Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement**|Michael Bloesch et.al.|[2507.06701](https://arxiv.org/abs/2507.06701)|null|\n"}, "Robotic Navigation": {"2508.05634": "|**2025-08-07**|**Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling**|Jianpeng Yao et.al.|[2508.05634](https://arxiv.org/abs/2508.05634)|null|\n", "2508.05616": "|**2025-08-07**|**TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution**|Zhikai Zhao et.al.|[2508.05616](https://arxiv.org/abs/2508.05616)|null|\n", "2508.05543": "|**2025-08-07**|**CleanUpBench: Embodied Sweeping and Grasping Benchmark**|Wenbo Li et.al.|[2508.05543](https://arxiv.org/abs/2508.05543)|null|\n", "2508.05312": "|**2025-08-07**|**Flow-driven magnetic microcatheter for superselective arterial embolization**|Lucio Pancaldi et.al.|[2508.05312](https://arxiv.org/abs/2508.05312)|null|\n", "2508.05253": "|**2025-08-07**|**Congestion Mitigation Path Planning for Large-Scale Multi-Agent Navigation in Dense Environments**|Takuro Kato et.al.|[2508.05253](https://arxiv.org/abs/2508.05253)|**[link](https://github.com/jyyang621/DailyArXiv)**|\n", "2508.05237": "|**2025-08-07**|**Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models**|Zane Xu et.al.|[2508.05237](https://arxiv.org/abs/2508.05237)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|\n", "2508.05205": "|**2025-08-07**|**EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery**|Bingyu Yang et.al.|[2508.05205](https://arxiv.org/abs/2508.05205)|**[link](https://github.com/Tavish9/awesome-daily-AI-arxiv)**|\n", "2508.05129": "|**2025-08-07**|**Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning**|Wuqiang Zheng et.al.|[2508.05129](https://arxiv.org/abs/2508.05129)|**[link](https://github.com/zachysun/DailyArXiv)**|\n", "2508.05081": "|**2025-08-07**|**Cognitive Duality for Adaptive Web Agents**|Jiarun Liu et.al.|[2508.05081](https://arxiv.org/abs/2508.05081)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|\n", "2508.05021": "|**2025-08-07**|**MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding**|Weifan Zhang et.al.|[2508.05021](https://arxiv.org/abs/2508.05021)|null|\n", "2508.04994": "|**2025-08-07**|**Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots**|Wenjie Hu et.al.|[2508.04994](https://arxiv.org/abs/2508.04994)|null|\n", "2508.04927": "|**2025-08-06**|**Engineering Topological Materials**|Amit Goft et.al.|[2508.04927](https://arxiv.org/abs/2508.04927)|**[link](https://github.com/ExpectozJJ/PF-OIHP)**|\n", "2508.04921": "|**2025-08-06**|**Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities**|Zixuan Feng et.al.|[2508.04921](https://arxiv.org/abs/2508.04921)|null|\n", "2508.04901": "|**2025-08-06**|**Sensitivity of Stability: Theoretical & Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning**|Prabhav Singh et.al.|[2508.04901](https://arxiv.org/abs/2508.04901)|null|\n", "2508.04821": "|**2025-08-06**|**At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp**|Yang Liu et.al.|[2508.04821](https://arxiv.org/abs/2508.04821)|null|\n", "2508.04678": "|**2025-08-06**|**Open Scene Graphs for Open-World Object-Goal Navigation**|Joel Loo et.al.|[2508.04678](https://arxiv.org/abs/2508.04678)|null|\n", "2508.04618": "|**2025-08-06**|**HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs**|Dengzhao Fang et.al.|[2508.04618](https://arxiv.org/abs/2508.04618)|null|\n", "2508.04598": "|**2025-08-06**|**$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything**|Lingfeng Zhang et.al.|[2508.04598](https://arxiv.org/abs/2508.04598)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2508.04459": "|**2025-08-06**|**Case Studies of Generative Machine Learning Models for Dynamical Systems**|Nachiket U. Bapat et.al.|[2508.04459](https://arxiv.org/abs/2508.04459)|**[link](https://github.com/CYandYue/Auto-Get-Papers-pro)**|\n", "2508.04372": "|**2025-08-06**|**Tactile Comfort: Lowering Heart Rate Through Interactions**|Morten Roed Frederiksen et.al.|[2508.04372](https://arxiv.org/abs/2508.04372)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2508.04326": "|**2025-08-07**|**Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research**|Ke Li et.al.|[2508.04326](https://arxiv.org/abs/2508.04326)|**[link](https://github.com/longxiang-ai/awesome-gaussians)**|\n", "2508.04280": "|**2025-08-06**|**Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success**|George Bredis et.al.|[2508.04280](https://arxiv.org/abs/2508.04280)|**[link](https://github.com/gabrielchua/daily-ai-papers)**|\n", "2508.04202": "|**2025-08-06**|**Unplug, Mute, Avoid: Investigating smart speaker users' privacy protection behaviours in Saudi Homes**|Abdulrhman Alorini et.al.|[2508.04202](https://arxiv.org/abs/2508.04202)|**[link](https://github.com/mitchelllisle/data-privacy-papers)**|\n", "2508.04131": "|**2025-08-06**|**DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation**|Zhaohong Huang et.al.|[2508.04131](https://arxiv.org/abs/2508.04131)|**[link](https://github.com/zhouchanggeng/DailyArXiv)**|\n", "2508.04025": "|**2025-08-06**|**Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement**|Chao Hao et.al.|[2508.04025](https://arxiv.org/abs/2508.04025)|**[link](https://github.com/ZJU-REAL/Awesome-GUI-Agents)**|\n", "2508.03890": "|**2025-08-07**|**Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes**|Sanghun Jung et.al.|[2508.03890](https://arxiv.org/abs/2508.03890)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2508.03672": "|**2025-08-05**|**Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways**|Zhongbi Luo et.al.|[2508.03672](https://arxiv.org/abs/2508.03672)|null|\n", "2508.03670": "|**2025-08-05**|**Personalized Recommendation of Dish and Restaurant Collections on iFood**|Fernando F. Granado et.al.|[2508.03670](https://arxiv.org/abs/2508.03670)|null|\n", "2508.03600": "|**2025-08-05**|**Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control**|Hamze Hammami et.al.|[2508.03600](https://arxiv.org/abs/2508.03600)|null|\n", "2508.03541": "|**2025-08-05**|**Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions**|Ergi Tushe et.al.|[2508.03541](https://arxiv.org/abs/2508.03541)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|\n", "2508.03425": "|**2025-08-05**|**Investigating a Characteristic Time Lag in the Ionospheric F-region's Response to Solar Flares**|Aisling N. O'Hare et.al.|[2508.03425](https://arxiv.org/abs/2508.03425)|null|\n", "2508.03777": "|**2025-08-05**|**When Agents Break Down in Multiagent Path Finding**|Foivos Fioravantes et.al.|[2508.03777](https://arxiv.org/abs/2508.03777)|**[link](https://github.com/jyyang621/DailyArXiv)**|\n", "2508.03410": "|**2025-08-05**|**VisAug: Facilitating Speech-Rich Web Video Navigation and Engagement with Auto-Generated Visual Augmentations**|Baoquan Zhao et.al.|[2508.03410](https://arxiv.org/abs/2508.03410)|null|\n", "2508.03408": "|**2025-08-06**|**Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments**|Ivana Collado-Gonzalez et.al.|[2508.03408](https://arxiv.org/abs/2508.03408)|null|\n", "2508.03350": "|**2025-08-05**|**Investigation of Air Fluidization during Intruder Penetration in Sand**|Bowen Wang et.al.|[2508.03350](https://arxiv.org/abs/2508.03350)|null|\n", "2508.03246": "|**2025-08-05**|**Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots**|Zehua Fan et.al.|[2508.03246](https://arxiv.org/abs/2508.03246)|**[link](https://github.com/Ed1sonChen/DailyArxiv)**|\n", "2508.03220": "|**2025-08-05**|**Timing is everything: How subtle timing changes in MRI echo planar imaging can significantly alter mechanical vibrations and sound level**|Amir Seginer et.al.|[2508.03220](https://arxiv.org/abs/2508.03220)|null|\n", "2508.03216": "|**2025-08-05**|**Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse**|Hikari Yanagawa et.al.|[2508.03216](https://arxiv.org/abs/2508.03216)|**[link](https://github.com/jyyang621/DailyArXiv)**|\n", "2508.03182": "|**2025-08-05**|**StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation**|Sangho Suh et.al.|[2508.03182](https://arxiv.org/abs/2508.03182)|null|\n", "2508.03138": "|**2025-08-05**|**Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation**|Mintaek Oh et.al.|[2508.03138](https://arxiv.org/abs/2508.03138)|null|\n", "2508.03129": "|**2025-08-05**|**Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection**|Le Qiu et.al.|[2508.03129](https://arxiv.org/abs/2508.03129)|null|\n", "2508.03068": "|**2025-08-07**|**Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching**|Sirui Chen et.al.|[2508.03068](https://arxiv.org/abs/2508.03068)|null|\n", "2508.03061": "|**2025-08-07**|**Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling**|Shuchang Xu et.al.|[2508.03061](https://arxiv.org/abs/2508.03061)|null|\n", "2508.03053": "|**2025-08-05**|**SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps**|Haojun Xu et.al.|[2508.03053](https://arxiv.org/abs/2508.03053)|null|\n", "2508.03027": "|**2025-08-05**|**CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction**|Yizhuo Wang et.al.|[2508.03027](https://arxiv.org/abs/2508.03027)|**[link](https://github.com/marmotlab/CogniPlan)**|\n", "2508.03012": "|**2025-08-06**|**Tool-integrated Reinforcement Learning for Repo Deep Search**|Zexiong Ma et.al.|[2508.03012](https://arxiv.org/abs/2508.03012)|**[link](https://github.com/gabrielchua/daily-ai-papers)**|\n", "2508.03000": "|**2025-08-05**|**SustainableQA: A Comprehensive Question Answering Dataset for Corporate Sustainability and EU Taxonomy Reporting**|Mohammed Ali et.al.|[2508.03000](https://arxiv.org/abs/2508.03000)|**[link](https://github.com/DataScienceUIBK/SustainableQA)**|\n", "2508.02988": "|**2025-08-05**|**GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring**|Linji Wang et.al.|[2508.02988](https://arxiv.org/abs/2508.02988)|**[link](https://github.com/linjiw/GACL)**|\n", "2508.02979": "|**2025-08-05**|**Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling**|Peng Ding et.al.|[2508.02979](https://arxiv.org/abs/2508.02979)|null|\n", "2508.02917": "|**2025-08-04**|**Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces**|Vebj\u00f8rn Haug K\u00e5sene et.al.|[2508.02917](https://arxiv.org/abs/2508.02917)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.02868": "|**2025-08-04**|**Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators**|Kaixuan Wang et.al.|[2508.02868](https://arxiv.org/abs/2508.02868)|null|\n", "2508.02617": "|**2025-08-04**|**Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An Imitation Learning Approach**|Peng Wei et.al.|[2508.02617](https://arxiv.org/abs/2508.02617)|null|\n", "2508.02610": "|**2025-08-04**|**PunchPulse: A Physically Demanding Virtual Reality Boxing Game Designed with, for and by Blind and Low-Vision Players**|Sanchita S. Kamath et.al.|[2508.02610](https://arxiv.org/abs/2508.02610)|null|\n", "2508.02601": "|**2025-08-04**|**StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes**|Siyi Liu et.al.|[2508.02601](https://arxiv.org/abs/2508.02601)|null|\n", "2508.02549": "|**2025-08-04**|**MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming**|Shuo Wang et.al.|[2508.02549](https://arxiv.org/abs/2508.02549)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|\n", "2508.02413": "|**2025-08-04**|**Improving Knowledge Graph Understanding with Contextual Views**|Antrea Christou et.al.|[2508.02413](https://arxiv.org/abs/2508.02413)|null|\n", "2508.02338": "|**2025-08-04**|**Vision Language Model-based Testing of Industrial Autonomous Mobile Robots**|Jiahui Wu et.al.|[2508.02338](https://arxiv.org/abs/2508.02338)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|\n", "2508.02287": "|**2025-08-04**|**Framework for Robust Motion Planning of Tethered Multi-Robot Systems in Marine Environments**|Markus Buchholz et.al.|[2508.02287](https://arxiv.org/abs/2508.02287)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|\n", "2508.02168": "|**2025-08-05**|**After the Party: Navigating the Mapping From Color to Ambient Lighting**|Florin-Alexandru Vasluianu et.al.|[2508.02168](https://arxiv.org/abs/2508.02168)|null|\n", "2508.02085": "|**2025-08-07**|**SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents**|Jiaye Lin et.al.|[2508.02085](https://arxiv.org/abs/2508.02085)|null|\n"}}