## Updated on 2025.08.12
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#robot-&-agent>Robot & Agent</a></li>
    <li><a href=#robotic-manipulation>Robotic Manipulation</a></li>
    <li><a href=#vision-language-action-model>Vision Language Action Model</a></li>
    <li><a href=#imitation-learning>Imitation Learning</a></li>
    <li><a href=#robotic-navigation>Robotic Navigation</a></li>
  </ol>
</details>

## Robot & Agent

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-08-08**|**Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation**|Youguang Xing et.al.|[2508.06426](https://arxiv.org/abs/2508.06426)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-04**|**A "good regulator theorem" for embodied agents**|Nathaniel Virgo et.al.|[2508.06326](https://arxiv.org/abs/2508.06326)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|
|**2025-08-07**|**Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling**|Jianpeng Yao et.al.|[2508.05634](https://arxiv.org/abs/2508.05634)|**[link](https://github.com/wonderNefelibata/Awesome-LRM-Safety)**|
|**2025-08-07**|**OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks**|Zixuan Wang et.al.|[2508.05614](https://arxiv.org/abs/2508.05614)|**[link](https://huggingface.co/datasets/wangzx1210/OmniEAR)**|
|**2025-08-07**|**CleanUpBench: Embodied Sweeping and Grasping Benchmark**|Wenbo Li et.al.|[2508.05543](https://arxiv.org/abs/2508.05543)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction**|Sahar Salimpour et.al.|[2508.05294](https://arxiv.org/abs/2508.05294)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-05**|**DiWA: Diffusion Policy Adaptation with World Models**|Akshay L Chandra et.al.|[2508.03645](https://arxiv.org/abs/2508.03645)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-08-07**|**Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching**|Sirui Chen et.al.|[2508.03068](https://arxiv.org/abs/2508.03068)|**[link](https://github.com/YanjieZe/awesome-humanoid-robot-learning)**|
|**2025-08-04**|**Learning User Interaction Forces using Vision for a Soft Finger Exosuit**|Mohamed Irfan Refai et.al.|[2508.02870](https://arxiv.org/abs/2508.02870)|null|
|**2025-08-06**|**HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents**|Yibin Liu et.al.|[2508.02629](https://arxiv.org/abs/2508.02629)|null|
|**2025-08-04**|**Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction**|Matus Krajcovic et.al.|[2508.02376](https://arxiv.org/abs/2508.02376)|**[link](https://github.com/zhengzangw/DailyArXiv)**|
|**2025-08-04**|**ScrewSplat: An End-to-End Method for Articulated Object Recognition**|Seungyeon Kim et.al.|[2508.02146](https://arxiv.org/abs/2508.02146)|**[link](https://github.com/longxiang-ai/awesome-gaussians)**|
|**2025-08-04**|**RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models**|Kaustubh Sridhar et.al.|[2508.02062](https://arxiv.org/abs/2508.02062)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-03**|**Learning to Perform Low-Contact Autonomous Nasotracheal Intubation by Recurrent Action-Confidence Chunking with Transformer**|Yu Tian et.al.|[2508.01808](https://arxiv.org/abs/2508.01808)|null|
|**2025-08-05**|**VPN: Visual Prompt Navigation**|Shuo Feng et.al.|[2508.01766](https://arxiv.org/abs/2508.01766)|**[link](https://github.com/farlit/VPN)**|
|**2025-08-03**|**OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping**|Danyang Li et.al.|[2508.01723](https://arxiv.org/abs/2508.01723)|null|
|**2025-08-03**|**DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding**|Hanqing Wang et.al.|[2508.01651](https://arxiv.org/abs/2508.01651)|null|
|**2025-08-03**|**CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation**|Sung-Wook Lee et.al.|[2508.01600](https://arxiv.org/abs/2508.01600)|null|
|**2025-08-02**|**COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning**|Sateesh Kumar et.al.|[2508.01131](https://arxiv.org/abs/2508.01131)|null|
|**2025-08-01**|**Video Generators are Robot Policies**|Junbang Liang et.al.|[2508.00795](https://arxiv.org/abs/2508.00795)|null|
|**2025-08-01**|**Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking**|Haoyu Wang et.al.|[2508.00500](https://arxiv.org/abs/2508.00500)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|
|**2025-08-01**|**Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents**|Janika Deborah Gajo et.al.|[2508.00400](https://arxiv.org/abs/2508.00400)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|
|**2025-08-01**|**Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging**|Tianshuang Qiu et.al.|[2508.00354](https://arxiv.org/abs/2508.00354)|**[link](https://github.com/liliu-avril/Awesome-Segment-Anything)**|
|**2025-07-31**|**SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting**|Di Li et.al.|[2507.23772](https://arxiv.org/abs/2507.23772)|**[link](https://github.com/hq-King/Awesome-Affordance-Learning)**|
|**2025-08-01**|**H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation**|Hongzhe Bi et.al.|[2507.23523](https://arxiv.org/abs/2507.23523)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-31**|**Policy Learning from Large Vision-Language Model Feedback without Reward Modeling**|Tung M. Luu et.al.|[2507.23391](https://arxiv.org/abs/2507.23391)|**[link](https://github.com/tunglm2203/plare)**|
|**2025-07-29**|**From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning**|Honglin He et.al.|[2507.22028](https://arxiv.org/abs/2507.22028)|**[link](https://github.com/ai4ce/CityWalker)**|
|**2025-07-29**|**DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments**|Yufei Jia et.al.|[2507.21981](https://arxiv.org/abs/2507.21981)|**[link](https://github.com/TATP-233/DISCOVERSE)**|
|**2025-07-29**|**MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects**|Yuying Zhang et.al.|[2507.21796](https://arxiv.org/abs/2507.21796)|null|
|**2025-08-03**|**Learning Physical Interaction Skills from Human Demonstrations**|Tianyu Li et.al.|[2507.20445](https://arxiv.org/abs/2507.20445)|null|
|**2025-07-26**|**Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models**|Anjali R. Menon et.al.|[2507.19854](https://arxiv.org/abs/2507.19854)|null|
|**2025-07-26**|**Moving Out: Physically-grounded Human-AI Collaboration**|Xuhui Kang et.al.|[2507.18623](https://arxiv.org/abs/2507.18623)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-07-24**|**EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs**|Yuping He et.al.|[2507.18342](https://arxiv.org/abs/2507.18342)|**[link](https://github.com/InternRobotics/InternSR)**|
|**2025-07-23**|**ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents**|Chang Nie et.al.|[2507.17462](https://arxiv.org/abs/2507.17462)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|
|**2025-07-29**|**VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback**|Jianxin Bi et.al.|[2507.17294](https://arxiv.org/abs/2507.17294)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-25**|**Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning**|Po-Yen Wu et.al.|[2507.17275](https://arxiv.org/abs/2507.17275)|null|
|**2025-07-23**|**Towards Human-level Intelligence via Human-like Whole-Body Manipulation**|Guang Gao et.al.|[2507.17141](https://arxiv.org/abs/2507.17141)|null|
|**2025-07-22**|**Benchmarking LLM Privacy Recognition for Social Robot Decision Making**|Dakota Sullivan et.al.|[2507.16124](https://arxiv.org/abs/2507.16124)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|
|**2025-07-21**|**Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers**|Ian Chuang et.al.|[2507.15833](https://arxiv.org/abs/2507.15833)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-22**|**GR-3 Technical Report**|Chilam Cheang et.al.|[2507.15493](https://arxiv.org/abs/2507.15493)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-21**|**EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent**|Jiaao Li et.al.|[2507.15428](https://arxiv.org/abs/2507.15428)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|
|**2025-07-20**|**Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions**|Majid Roshanfar et.al.|[2507.15155](https://arxiv.org/abs/2507.15155)|null|
|**2025-07-20**|**TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP**|Fan Li et.al.|[2507.14904](https://arxiv.org/abs/2507.14904)|**[link](https://github.com/Lyz103/LLM-Agent-Paper-daily)**|
|**2025-07-17**|**Latent Policy Steering with Embodiment-Agnostic Pretrained World Models**|Yiqi Wang et.al.|[2507.13340](https://arxiv.org/abs/2507.13340)|null|
|**2025-07-17**|**Learning to Predict Mobile Robot Stability in Off-Road Environments**|Nathaniel Rose et.al.|[2507.12731](https://arxiv.org/abs/2507.12731)|null|
|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al.|[2507.12440](https://arxiv.org/abs/2507.12440)|null|
|**2025-07-15**|**EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks**|Joohwan Seo et.al.|[2507.10961](https://arxiv.org/abs/2507.10961)|null|
|**2025-07-14**|**Bridging Brains and Machines: A Unified Frontier in Neuroscience, Artificial Intelligence, and Neuromorphic Systems**|Sohan Shankar et.al.|[2507.10722](https://arxiv.org/abs/2507.10722)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|
|**2025-07-29**|**MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation**|Juyi Sheng et.al.|[2507.10543](https://arxiv.org/abs/2507.10543)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|
|**2025-07-16**|**MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping**|Obaidullah Zaland et.al.|[2507.10158](https://arxiv.org/abs/2507.10158)|null|
|**2025-07-13**|**Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling**|Ali Safa et.al.|[2507.09540](https://arxiv.org/abs/2507.09540)|**[link](https://github.com/SpikingChen/SNN-Daily-Arxiv)**|
|**2025-07-15**|**Learning and Transferring Better with Depth Information in Visual Reinforcement Learning**|Zichun Xu et.al.|[2507.09180](https://arxiv.org/abs/2507.09180)|null|
|**2025-07-12**|**Towards Human-level Dexterity via Robot Learning**|Gagan Khandate et.al.|[2507.09117](https://arxiv.org/abs/2507.09117)|null|
|**2025-07-26**|**Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction**|Thomas T. Zhang et.al.|[2507.09061](https://arxiv.org/abs/2507.09061)|null|
|**2025-07-11**|**Learning human-to-robot handovers through 3D scene reconstruction**|Yuekun Wu et.al.|[2507.08726](https://arxiv.org/abs/2507.08726)|null|
|**2025-07-11**|**Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots**|Yang Zhang et.al.|[2507.08303](https://arxiv.org/abs/2507.08303)|null|
|**2025-07-09**|**LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation**|Sonia Raychaudhuri et.al.|[2507.07299](https://arxiv.org/abs/2507.07299)|**[link](https://github.com/Lyz103/LLM-Agent-Paper-daily)**|
|**2025-07-08**|**Is Diversity All You Need for Scalable Robotic Manipulation?**|Modi Shi et.al.|[2507.06219](https://arxiv.org/abs/2507.06219)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-08**|**Learning Agile Tensile Perching for Aerial Robots from Demonstrations**|Kangle Yuan et.al.|[2507.06172](https://arxiv.org/abs/2507.06172)|**[link](https://github.com/AerialRoboticsGroup/agile-tethered-perching)**|
|**2025-07-08**|**Conditional Multi-Stage Failure Recovery for Embodied Agents**|Youmna Farag et.al.|[2507.06016](https://arxiv.org/abs/2507.06016)|**[link](https://github.com/AGI-Edgerunners/LLM-Agents-Papers)**|
|**2025-07-08**|**Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning**|Xiatao Sun et.al.|[2507.05695](https://arxiv.org/abs/2507.05695)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-07**|**EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling**|Boyuan Wang et.al.|[2507.05198](https://arxiv.org/abs/2507.05198)|**[link](https://github.com/leofan90/Awesome-World-Models)**|

<p align=right>(<a href=#updated-on-20250812>back to top</a>)</p>

## Robotic Manipulation

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-08-08**|**When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via Telemetry Manipulation**|Dario Pasquini et.al.|[2508.06394](https://arxiv.org/abs/2508.06394)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|
|**2025-08-08**|**Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators**|Amir Hossein Barjini et.al.|[2508.06313](https://arxiv.org/abs/2508.06313)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**A Tensor Train Approach for Deterministic Arithmetic Operations on Discrete Representations of Probability Distributions**|Gerhard Kirsten et.al.|[2508.06303](https://arxiv.org/abs/2508.06303)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Real-Time 3D Vision-Language Embedding Mapping**|Christian Rauch et.al.|[2508.06291](https://arxiv.org/abs/2508.06291)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators**|Juan Heredia et.al.|[2508.06276](https://arxiv.org/abs/2508.06276)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints**|Zezeng Li et.al.|[2508.06266](https://arxiv.org/abs/2508.06266)|null|
|**2025-08-08**|**Deepfake Detection that Generalizes Across Benchmarks**|Andrii Yermakov et.al.|[2508.06248](https://arxiv.org/abs/2508.06248)|null|
|**2025-08-08**|**Correlation between Exciton Dynamics and Spin Structure in van der Waals Antiferromagnet NiPS3**|Kang Wang et.al.|[2508.06246](https://arxiv.org/abs/2508.06246)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Reparameterization Proximal Policy Optimization**|Hai Zhong et.al.|[2508.06214](https://arxiv.org/abs/2508.06214)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model**|Hanqing Wang et.al.|[2508.06206](https://arxiv.org/abs/2508.06206)|**[link](https://github.com/hq-King/Awesome-Affordance-Learning)**|
|**2025-08-08**|**Enhancing Plasmonic Superconductivity in Layered Materials via Dynamical Coulomb Engineering**|Yann in 't Veld et.al.|[2508.06195](https://arxiv.org/abs/2508.06195)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Finite Length Effects and Coulomb Interaction in Ge Quantum Well-Based Josephson Junctions Probed with Microwave Spectroscopy**|S. C. ten Kate et.al.|[2508.06180](https://arxiv.org/abs/2508.06180)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning**|Xing Lei et.al.|[2508.06108](https://arxiv.org/abs/2508.06108)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization**|Yachun Mi et.al.|[2508.06101](https://arxiv.org/abs/2508.06101)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Bounding Distributional Shifts in World Modeling through Novelty Detection**|Eric Jing et.al.|[2508.06096](https://arxiv.org/abs/2508.06096)|**[link](https://github.com/leofan90/Awesome-World-Models)**|
|**2025-08-08**|**Incremental Language Understanding for Online Motion Planning of Robot Manipulators**|Mitchell Abrams et.al.|[2508.06095](https://arxiv.org/abs/2508.06095)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**DreamVE: Unified Instruction-based Image and Video Editing**|Bin Xia et.al.|[2508.06080](https://arxiv.org/abs/2508.06080)|null|
|**2025-08-08**|**ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation**|Daniel Lee et.al.|[2508.06065](https://arxiv.org/abs/2508.06065)|null|
|**2025-08-08**|**Don't Forget Imagination!**|Evgenii E. Vityaev et.al.|[2508.06062](https://arxiv.org/abs/2508.06062)|null|
|**2025-08-08**|**PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation**|Zhihao Zhu et.al.|[2508.05976](https://arxiv.org/abs/2508.05976)|null|
|**2025-08-08**|**Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution**|Zhanyi Sun et.al.|[2508.05941](https://arxiv.org/abs/2508.05941)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts**|Gen Sako et.al.|[2508.05937](https://arxiv.org/abs/2508.05937)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Acoustic wave modulation of gap plasmon cavities**|Skyler P. Selvin et.al.|[2508.05864](https://arxiv.org/abs/2508.05864)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Rydberg Exciton Dynamics in the Blockade Regime of Cu2O**|Gillian E. Minarik et.al.|[2508.05806](https://arxiv.org/abs/2508.05806)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**A Framework for Inherently Safer AGI through Language-Mediated Active Inference**|Bo Wen et.al.|[2508.05766](https://arxiv.org/abs/2508.05766)|**[link](https://github.com/wonderNefelibata/Awesome-LRM-Safety)**|
|**2025-08-07**|**Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation**|Yue Liao et.al.|[2508.05635](https://arxiv.org/abs/2508.05635)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-08-07**|**How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations**|Brandon Jaipersaud et.al.|[2508.05625](https://arxiv.org/abs/2508.05625)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Back to Bits: Extending Shannon's communication performance framework to computing**|Max Hawkins et.al.|[2508.05621](https://arxiv.org/abs/2508.05621)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**The Missing Reward: Active Inference in the Era of Experience**|Bo Wen et.al.|[2508.05619](https://arxiv.org/abs/2508.05619)|null|
|**2025-08-07**|**Ultra-Large-Scale Compilation and Manipulation of Quantum Circuits with Pandora**|Ioana Moflic et.al.|[2508.05608](https://arxiv.org/abs/2508.05608)|null|
|**2025-08-07**|**Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator**|Van Cuong Pham et.al.|[2508.05584](https://arxiv.org/abs/2508.05584)|null|
|**2025-08-07**|**CleanUpBench: Embodied Sweeping and Grasping Benchmark**|Wenbo Li et.al.|[2508.05543](https://arxiv.org/abs/2508.05543)|null|
|**2025-08-07**|**Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation**|Albert Yu et.al.|[2508.05535](https://arxiv.org/abs/2508.05535)|**[link](https://github.com/UT-Austin-RobIn/MicoBot)**|
|**2025-08-07**|**When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework**|Haoyu Liu et.al.|[2508.05526](https://arxiv.org/abs/2508.05526)|**[link](https://github.com/zezhishao/DailyArXiv)**|
|**2025-08-07**|**Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes**|Zachary Robertson et.al.|[2508.05469](https://arxiv.org/abs/2508.05469)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|
|**2025-08-07**|**Do Robots Really Need Anthropomorphic Hands?**|Alexander Fabisch et.al.|[2508.05415](https://arxiv.org/abs/2508.05415)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization**|Farah Wahida et.al.|[2508.05409](https://arxiv.org/abs/2508.05409)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|
|**2025-08-07**|**Real-Time Iteration Scheme for Diffusion Policy**|Yufei Duan et.al.|[2508.05396](https://arxiv.org/abs/2508.05396)|**[link](https://huggingface.co/models/duandaxia/rti-dp-scale)**|
|**2025-08-07**|**Symmetry Packaging I: Irreducible Representation Blocks, Superselection, and Packaged Entanglement in Quantum Field Theory**|Rongchao Ma et.al.|[2508.05356](https://arxiv.org/abs/2508.05356)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control**|Shunlei Li et.al.|[2508.05342](https://arxiv.org/abs/2508.05342)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-07**|**ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning**|Jelle Luijkx et.al.|[2508.05310](https://arxiv.org/abs/2508.05310)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Learning to See and Act: Task-Aware View Planning for Robotic Manipulation**|Yongjie Bai et.al.|[2508.05186](https://arxiv.org/abs/2508.05186)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-07**|**FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction**|Mohammed Daba et.al.|[2508.05153](https://arxiv.org/abs/2508.05153)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Metacognition and self-regulated learning in manipulative robotic problem-solving task**|Margarida Romero et.al.|[2508.05112](https://arxiv.org/abs/2508.05112)|null|
|**2025-08-07**|**MetaDiT: Enabling Fine-grained Constraints in High-degree-of Freedom Metasurface Design**|Hao Li et.al.|[2508.05076](https://arxiv.org/abs/2508.05076)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System**|Boyang Zhang et.al.|[2508.05040](https://arxiv.org/abs/2508.05040)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Wave Computing based on Dynamical Networks: Applications in Optimization Problems**|Yunwen Liu et.al.|[2508.05014](https://arxiv.org/abs/2508.05014)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-06**|**INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM**|Jin Wang et.al.|[2508.04931](https://arxiv.org/abs/2508.04931)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-06**|**Fault-Tolerant Universal Quantum Computing in the Presence of Anisotropic Noise**|Yang-Yang Xie et.al.|[2508.04892](https://arxiv.org/abs/2508.04892)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-06**|**An Implementation of a Visual Stepper in the GRASP Programming System**|Panicz Maciej Godek et.al.|[2508.04859](https://arxiv.org/abs/2508.04859)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-06**|**Modelling the emergence of open-ended technological evolution**|James Winters et.al.|[2508.04828](https://arxiv.org/abs/2508.04828)|**[link](https://github.com/j-winters/bitw0r1d)**|
|**2025-08-06**|**At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp**|Yang Liu et.al.|[2508.04821](https://arxiv.org/abs/2508.04821)|null|
|**2025-08-06**|**Tight-binding photonics**|Jing Li et.al.|[2508.04465](https://arxiv.org/abs/2508.04465)|null|
|**2025-08-06**|**Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling**|Yongyi Wang et.al.|[2508.04282](https://arxiv.org/abs/2508.04282)|null|
|**2025-08-06**|**Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy**|Jairo Gudiño-Rosero et.al.|[2508.04281](https://arxiv.org/abs/2508.04281)|null|
|**2025-08-06**|**A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models**|Jiayi Wen et.al.|[2508.04276](https://arxiv.org/abs/2508.04276)|null|
|**2025-08-06**|**Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models**|Siddhant Panpatil et.al.|[2508.04196](https://arxiv.org/abs/2508.04196)|**[link](https://github.com/shaokangW/LLM-wisdom)**|
|**2025-08-06**|**$\boldsymbol{d}$-vector precession induced pumping in topological $p$ -wave superconductors**|Jun-Jie Fu et.al.|[2508.04188](https://arxiv.org/abs/2508.04188)|null|
|**2025-08-06**|**VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning**|Yuheng Ji et.al.|[2508.04043](https://arxiv.org/abs/2508.04043)|**[link](https://huggingface.co/datasets/wyp-ucas/VisualTrans)**|
|**2025-08-06**|**Identity Theft in AI Conference Peer Review**|Nihar B. Shah et.al.|[2508.04024](https://arxiv.org/abs/2508.04024)|null|
|**2025-08-06**|**Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)**|Vu Ngoc Son et.al.|[2508.04009](https://arxiv.org/abs/2508.04009)|null|
|**2025-08-05**|**Sub-5-fs compression and synchronization of relativistic electron bunches enabled by a high-gradient $α$ -magnet and low-jitter photoinjector**|Yining Yang et.al.|[2508.03946](https://arxiv.org/abs/2508.03946)|null|
|**2025-08-05**|**Constraint-Preserving Data Generation for Visuomotor Policy Learning**|Kevin Lin et.al.|[2508.03944](https://arxiv.org/abs/2508.03944)|null|
|**2025-08-08**|**CoAct-1: Computer-using Agents with Coding as Actions**|Linxin Song et.al.|[2508.03923](https://arxiv.org/abs/2508.03923)|**[link](https://github.com/kyegomez/awesome-multi-agent-papers)**|
|**2025-08-05**|**Particle manipulation by hydrodynamic effects in vortical Stokes flow**|Xuchen Liu et.al.|[2508.03894](https://arxiv.org/abs/2508.03894)|null|
|**2025-08-05**|**AttnTrace: Attention-based Context Traceback for Long-Context LLMs**|Yanting Wang et.al.|[2508.03793](https://arxiv.org/abs/2508.03793)|**[link](https://huggingface.co/spaces/SecureLLMSys/AttnTrace)**|
|**2025-08-05**|**LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences**|Ao Liang et.al.|[2508.03692](https://arxiv.org/abs/2508.03692)|**[link](https://github.com/leofan90/Awesome-World-Models)**|
|**2025-08-05**|**DiWA: Diffusion Policy Adaptation with World Models**|Akshay L Chandra et.al.|[2508.03645](https://arxiv.org/abs/2508.03645)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-08-05**|**OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War**|Johannes Niu et.al.|[2508.03599](https://arxiv.org/abs/2508.03599)|null|
|**2025-08-06**|**EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable Text-to-Speech via Activation Steering**|Tianxin Xie et.al.|[2508.03543](https://arxiv.org/abs/2508.03543)|**[link](https://github.com/iszhanjiawei/flow_matching_arxiv_daily)**|
|**2025-08-05**|**CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation**|Kun Song et.al.|[2508.03526](https://arxiv.org/abs/2508.03526)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-06**|**Oxide Interface-Based Polymorphic Electronic Devices for Neuromorphic Computing**|Soumen Pradhan et.al.|[2508.03515](https://arxiv.org/abs/2508.03515)|null|
|**2025-08-05**|**Homogenization rates of beam lattices to micropolar continua**|Eric T. Chung et.al.|[2508.03512](https://arxiv.org/abs/2508.03512)|null|
|**2025-08-05**|**VideoGuard: Protecting Video Content from Unauthorized Editing**|Junjie Cao et.al.|[2508.03480](https://arxiv.org/abs/2508.03480)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|
|**2025-08-05**|**GRASPing Anatomy to Improve Pathology Segmentation**|Keyi Li et.al.|[2508.03374](https://arxiv.org/abs/2508.03374)|null|
|**2025-08-05**|**When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs**|Bodam Kim et.al.|[2508.03365](https://arxiv.org/abs/2508.03365)|null|
|**2025-08-05**|**Engineering subgap states in superconductors by altermagnetism**|Bo Lu et.al.|[2508.03364](https://arxiv.org/abs/2508.03364)|null|
|**2025-08-05**|**UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands**|Haoran Lin et.al.|[2508.03339](https://arxiv.org/abs/2508.03339)|**[link](https://github.com/haochen611/UFG)**|
|**2025-08-05**|**BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models**|Yu Pan et.al.|[2508.03221](https://arxiv.org/abs/2508.03221)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|
|**2025-08-05**|**ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow**|Shanshan Guo et.al.|[2508.03218](https://arxiv.org/abs/2508.03218)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-05**|**Can Large Language Models Identify Materials from Radar Signals?**|Jiangyou Zhu et.al.|[2508.03120](https://arxiv.org/abs/2508.03120)|null|
|**2025-08-05**|**Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation**|Zizhong Li et.al.|[2508.03110](https://arxiv.org/abs/2508.03110)|null|
|**2025-08-05**|**Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping**|Sang Min Kim et.al.|[2508.03099](https://arxiv.org/abs/2508.03099)|null|
|**2025-08-05**|**Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning**|Rui Pu et.al.|[2508.03054](https://arxiv.org/abs/2508.03054)|null|
|**2025-08-05**|**NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification**|Wenshuo Zhang et.al.|[2508.02823](https://arxiv.org/abs/2508.02823)|null|

<p align=right>(<a href=#updated-on-20250812>back to top</a>)</p>

## Vision Language Action Model

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-08-07**|**Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control**|Shunlei Li et.al.|[2508.05342](https://arxiv.org/abs/2508.05342)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-07**|**Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction**|Sahar Salimpour et.al.|[2508.05294](https://arxiv.org/abs/2508.05294)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-04**|**CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning**|Dongchi Huang et.al.|[2508.02219](https://arxiv.org/abs/2508.02219)|**[link](https://github.com/XiaoWei-i/Awesome-VLA-RL)**|
|**2025-08-04**|**RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models**|Kaustubh Sridhar et.al.|[2508.02062](https://arxiv.org/abs/2508.02062)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-07-31**|**XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation**|Zhigen Zhao et.al.|[2508.00097](https://arxiv.org/abs/2508.00097)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-07-31**|**villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models**|Xiaoyu Chen et.al.|[2507.23682](https://arxiv.org/abs/2507.23682)|**[link](https://huggingface.co/models/microsoft/villa-x)**|
|**2025-07-30**|**Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance**|Songsheng Wang et.al.|[2507.22424](https://arxiv.org/abs/2507.22424)|**[link](https://github.com/hemingkx/SpeculativeDecodingPapers)**|
|**2025-07-23**|**Confidence Calibration in Vision-Language-Action Models**|Thomas P Zollo et.al.|[2507.17383](https://arxiv.org/abs/2507.17383)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-29**|**VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback**|Jianxin Bi et.al.|[2507.17294](https://arxiv.org/abs/2507.17294)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-21**|**Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos**|Hao Luo et.al.|[2507.15597](https://arxiv.org/abs/2507.15597)|**[link](https://huggingface.co/models/BeingBeyond/Being-H0)**|
|**2025-07-18**|**EdgeVLA: Efficient Vision-Language-Action Models**|Paweł Budzianowski et.al.|[2507.14049](https://arxiv.org/abs/2507.14049)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al.|[2507.12440](https://arxiv.org/abs/2507.12440)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-14**|**Vision Language Action Models in Robotic Manipulation: A Systematic Review**|Muhayy Ud Din et.al.|[2507.10672](https://arxiv.org/abs/2507.10672)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-12**|**Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization**|Jialei Huang et.al.|[2507.09160](https://arxiv.org/abs/2507.09160)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-09**|**3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds**|Fan-Yun Sun et.al.|[2507.06484](https://arxiv.org/abs/2507.06484)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|
|**2025-07-07**|**NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving**|Qucheng Peng et.al.|[2507.05227](https://arxiv.org/abs/2507.05227)|**[link](https://github.com/runjtu/awesome-and-novel-works-in-slam)**|
|**2025-07-17**|**DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge**|Wenyao Zhang et.al.|[2507.04447](https://arxiv.org/abs/2507.04447)|**[link](https://huggingface.co/models/WenyaoZhang/DreamVLA)**|
|**2025-07-02**|**A Survey on Vision-Language-Action Models: An Action Tokenization Perspective**|Yifan Zhong et.al.|[2507.01925](https://arxiv.org/abs/2507.01925)|**[link](https://github.com/TianxingChen/Embodied-AI-Guide)**|
|**2025-07-02**|**MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics**|Dmytro Kuzmenko et.al.|[2507.01843](https://arxiv.org/abs/2507.01843)|null|
|**2025-07-03**|**TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control**|Zhenyang Liu et.al.|[2507.01424](https://arxiv.org/abs/2507.01424)|null|
|**2025-07-01**|**VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers**|Yating Wang et.al.|[2507.01016](https://arxiv.org/abs/2507.01016)|null|
|**2025-07-01**|**Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding**|Tao Lin et.al.|[2507.00416](https://arxiv.org/abs/2507.00416)|null|
|**2025-06-30**|**A Survey on Vision-Language-Action Models for Autonomous Driving**|Sicong Jiang et.al.|[2506.24044](https://arxiv.org/abs/2506.24044)|**[link](https://github.com/52CV/CV-Surveys)**|
|**2025-06-24**|**Unified Vision-Language-Action Model**|Yuqi Wang et.al.|[2506.19850](https://arxiv.org/abs/2506.19850)|**[link](https://huggingface.co/models/Yuqi1997/UniVLA)**|
|**2025-07-07**|**RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models**|Jacky Kwok et.al.|[2506.17811](https://arxiv.org/abs/2506.17811)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-06-21**|**RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models**|Yuxuan Chen et.al.|[2506.17639](https://arxiv.org/abs/2506.17639)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-06-21**|**VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models**|Chongkai Gao et.al.|[2506.17561](https://arxiv.org/abs/2506.17561)|**[link](https://huggingface.co/models/Linslab/VLA-OS)**|
|**2025-06-19**|**ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models**|Puhao Li et.al.|[2506.16211](https://arxiv.org/abs/2506.16211)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-06-19**|**ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes**|Zeyuan Chen et.al.|[2506.14317](https://arxiv.org/abs/2506.14317)|**[link](https://github.com/YanjieZe/3D-Diffusion-Policy)**|
|**2025-06-16**|**AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning**|Zewei Zhou et.al.|[2506.13757](https://arxiv.org/abs/2506.13757)|**[link](https://github.com/Thinklab-SJTU/Awesome-LLM4AD)**|
|**2025-06-16**|**CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding**|Wenxuan Song et.al.|[2506.13725](https://arxiv.org/abs/2506.13725)|**[link](https://huggingface.co/models/chenpyyy/openvla-ac)**|
|**2025-06-16**|**Block-wise Adaptive Caching for Accelerating Diffusion Policy**|Kangye Ji et.al.|[2506.13456](https://arxiv.org/abs/2506.13456)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-06-19**|**A Comprehensive Survey on Continual Learning in Generative Models**|Haiyang Guo et.al.|[2506.13045](https://arxiv.org/abs/2506.13045)|**[link](https://github.com/52CV/CV-Surveys)**|
|**2025-06-13**|**RationalVLA: A Rational Vision-Language-Action Model with Dual System**|Wenxuan Song et.al.|[2506.10826](https://arxiv.org/abs/2506.10826)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-06-11**|**EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models**|Yantai Yang et.al.|[2506.10100](https://arxiv.org/abs/2506.10100)|null|
|**2025-06-11**|**SAFE: Multitask Failure Detection for Vision-Language-Action Models**|Qiao Gu et.al.|[2506.09937](https://arxiv.org/abs/2506.09937)|**[link](https://github.com/vla-safe/SAFE)**|
|**2025-06-11**|**From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models**|Irving Fang et.al.|[2506.09930](https://arxiv.org/abs/2506.09930)|**[link](https://huggingface.co/models/IPEC-COMMUNITY/spatialvla-4b-224-sft-bridge)**|
|**2025-06-17**|**An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models**|Pranav Guruprasad et.al.|[2506.09172](https://arxiv.org/abs/2506.09172)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-06-11**|**TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization**|Zengjue Chen et.al.|[2506.08440](https://arxiv.org/abs/2506.08440)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-06-09**|**BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation**|Hongyu Wang et.al.|[2506.07530](https://arxiv.org/abs/2506.07530)|**[link](https://huggingface.co/models/hongyuw/bitvla-bitsiglipL-224px-bf16)**|
|**2025-06-09**|**Real-Time Execution of Action Chunking Flow Policies**|Kevin Black et.al.|[2506.07339](https://arxiv.org/abs/2506.07339)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-20**|**MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping**|Vineet Bhat et.al.|[2506.06535](https://arxiv.org/abs/2506.06535)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-06-04**|**SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models**|Meng Li et.al.|[2506.03574](https://arxiv.org/abs/2506.03574)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-06-03**|**Adversarial Attacks on Robotic Vision Language Action Models**|Eliot Krzysztof Jones et.al.|[2506.03350](https://arxiv.org/abs/2506.03350)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-06-02**|**SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics**|Mustafa Shukor et.al.|[2506.01844](https://arxiv.org/abs/2506.01844)|**[link](https://huggingface.co/spaces/arpitg1304/lerobot_scripts_simplified)**|
|**2025-06-02**|**ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding**|Yiyang Zhou et.al.|[2506.01300](https://arxiv.org/abs/2506.01300)|**[link](https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs)**|
|**2025-06-01**|**OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation**|Ishika Singh et.al.|[2506.01196](https://arxiv.org/abs/2506.01196)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-05-31**|**LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks**|Yi Yang et.al.|[2506.00411](https://arxiv.org/abs/2506.00411)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-05-29**|**Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models**|Haohan Chi et.al.|[2505.23757](https://arxiv.org/abs/2505.23757)|null|
|**2025-05-29**|**Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better**|Danny Driess et.al.|[2505.23705](https://arxiv.org/abs/2505.23705)|null|
|**2025-06-11**|**Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents**|Zhejian Yang et.al.|[2505.23450](https://arxiv.org/abs/2505.23450)|null|
|**2025-05-29**|**ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge**|Zhongyi Zhou et.al.|[2505.21906](https://arxiv.org/abs/2505.21906)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-31**|**EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models**|Feng Jiang et.al.|[2505.21567](https://arxiv.org/abs/2505.21567)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-07-08**|**Hume: Introducing System-2 Thinking in Visual-Language-Action Model**|Haoming Song et.al.|[2505.21432](https://arxiv.org/abs/2505.21432)|**[link](https://huggingface.co/models/Hume-vla/Hume-System2)**|
|**2025-05-27**|**Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models**|Xudong Tan et.al.|[2505.21200](https://arxiv.org/abs/2505.21200)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-05-26**|**Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review**|Matthew Lisondra et.al.|[2505.20503](https://arxiv.org/abs/2505.20503)|**[link](https://github.com/52CV/CV-Surveys)**|
|**2025-05-22**|**Interactive Post-Training for Vision-Language-Action Models**|Shuhan Tan et.al.|[2505.17016](https://arxiv.org/abs/2505.17016)|**[link](https://huggingface.co/models/tanshh97/RIPT_VLA)**|
|**2025-05-22**|**Perceptual Quality Assessment for Embodied AI**|Chunyi Li et.al.|[2505.16815](https://arxiv.org/abs/2505.16815)|**[link](https://github.com/chenin-wang/awesome_ai_paper)**|
|**2025-05-22**|**BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization**|Xueyang Zhou et.al.|[2505.16640](https://arxiv.org/abs/2505.16640)|**[link](https://github.com/GT-RIPL/Awesome-LLM-Robotics)**|
|**2025-05-22**|**DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving**|Zhenjie Yang et.al.|[2505.16278](https://arxiv.org/abs/2505.16278)|**[link](https://github.com/Thinklab-SJTU/Awesome-LLM4AD)**|

<p align=right>(<a href=#updated-on-20250812>back to top</a>)</p>

## Imitation Learning

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-08-08**|**Towards Balanced Behavior Cloning from Imbalanced Datasets**|Sagar Parekh et.al.|[2508.06319](https://arxiv.org/abs/2508.06319)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning**|Daechul Ahn et.al.|[2508.06042](https://arxiv.org/abs/2508.06042)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning**|Haohui Chen et.al.|[2508.05960](https://arxiv.org/abs/2508.05960)|**[link](https://github.com/Tavish9/awesome-daily-AI-arxiv)**|
|**2025-08-08**|**Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution**|Zhanyi Sun et.al.|[2508.05941](https://arxiv.org/abs/2508.05941)|null|
|**2025-08-07**|**ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning**|Jelle Luijkx et.al.|[2508.05310](https://arxiv.org/abs/2508.05310)|null|
|**2025-08-07**|**Cognitive Duality for Adaptive Web Agents**|Jiarun Liu et.al.|[2508.05081](https://arxiv.org/abs/2508.05081)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning**|Luai Abuelsamen et.al.|[2508.05077](https://arxiv.org/abs/2508.05077)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-05**|**Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection**|Le Qiu et.al.|[2508.03129](https://arxiv.org/abs/2508.03129)|**[link](https://github.com/dingyue772/DailyArxiv)**|
|**2025-08-05**|**Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control**|Yi-Hsuan Hsiao et.al.|[2508.03043](https://arxiv.org/abs/2508.03043)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|
|**2025-08-04**|**Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An Imitation Learning Approach**|Peng Wei et.al.|[2508.02617](https://arxiv.org/abs/2508.02617)|null|
|**2025-08-04**|**CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning**|Dongchi Huang et.al.|[2508.02219](https://arxiv.org/abs/2508.02219)|null|
|**2025-08-04**|**RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models**|Kaustubh Sridhar et.al.|[2508.02062](https://arxiv.org/abs/2508.02062)|null|
|**2025-08-03**|**CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation**|Sung-Wook Lee et.al.|[2508.01600](https://arxiv.org/abs/2508.01600)|null|
|**2025-08-02**|**Physically-based Lighting Augmentation for Robotic Manipulation**|Shutong Jin et.al.|[2508.01442](https://arxiv.org/abs/2508.01442)|null|
|**2025-08-02**|**T2S: Tokenized Skill Scaling for Lifelong Imitation Learning**|Hongquan Zhang et.al.|[2508.01167](https://arxiv.org/abs/2508.01167)|null|
|**2025-08-02**|**COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning**|Sateesh Kumar et.al.|[2508.01131](https://arxiv.org/abs/2508.01131)|null|
|**2025-08-01**|**Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation**|Ibrahim Althamary et.al.|[2508.01060](https://arxiv.org/abs/2508.01060)|null|
|**2025-08-01**|**Video Generators are Robot Policies**|Junbang Liang et.al.|[2508.00795](https://arxiv.org/abs/2508.00795)|null|
|**2025-08-01**|**On-Device Diffusion Transformer Policy for Efficient Robot Manipulation**|Yiming Wu et.al.|[2508.00697](https://arxiv.org/abs/2508.00697)|null|
|**2025-08-01**|**HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning**|Carlo Alessi et.al.|[2508.00491](https://arxiv.org/abs/2508.00491)|null|
|**2025-08-01**|**Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning**|Saichao Liu et.al.|[2508.00261](https://arxiv.org/abs/2508.00261)|**[link](https://github.com/zezhishao/DailyArXiv)**|
|**2025-08-01**|**H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation**|Hongzhe Bi et.al.|[2507.23523](https://arxiv.org/abs/2507.23523)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-08-10**|**In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion**|Yuanhao Chen et.al.|[2507.23053](https://arxiv.org/abs/2507.23053)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-07-30**|**Improving Generalization Ability of Robotic Imitation Learning by Resolving Causal Confusion in Observations**|Yifei Chen et.al.|[2507.22380](https://arxiv.org/abs/2507.22380)|null|
|**2025-07-29**|**RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation**|Dongyub Jude Lee et.al.|[2507.22219](https://arxiv.org/abs/2507.22219)|null|
|**2025-07-29**|**DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments**|Yufei Jia et.al.|[2507.21981](https://arxiv.org/abs/2507.21981)|**[link](https://github.com/TATP-233/DISCOVERSE)**|
|**2025-07-29**|**MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects**|Yuying Zhang et.al.|[2507.21796](https://arxiv.org/abs/2507.21796)|null|
|**2025-07-29**|**Model Predictive Adversarial Imitation Learning for Planning from Observation**|Tyler Han et.al.|[2507.21533](https://arxiv.org/abs/2507.21533)|null|
|**2025-07-29**|**Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training**|Sodtavilan Odonchimed et.al.|[2507.21452](https://arxiv.org/abs/2507.21452)|null|
|**2025-07-28**|**FMimic: Foundation Models are Fine-grained Action Learners from Human Videos**|Guangyan Chen et.al.|[2507.20622](https://arxiv.org/abs/2507.20622)|null|
|**2025-07-26**|**Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models**|Anjali R. Menon et.al.|[2507.19854](https://arxiv.org/abs/2507.19854)|null|
|**2025-07-26**|**Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation**|Ziyin Xiong et.al.|[2507.19817](https://arxiv.org/abs/2507.19817)|**[link](https://github.com/ziyin-xiong/Ag2x2)**|
|**2025-07-25**|**GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning**|Amin Banayeeanzade et.al.|[2507.19647](https://arxiv.org/abs/2507.19647)|**[link](https://github.com/nfbahrani/GABRIL-Atari)**|
|**2025-07-25**|**MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service**|Ming Gong et.al.|[2507.18884](https://arxiv.org/abs/2507.18884)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|
|**2025-07-24**|**Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning**|David Blanco-Mulero et.al.|[2507.18436](https://arxiv.org/abs/2507.18436)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-07-23**|**ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents**|Chang Nie et.al.|[2507.17462](https://arxiv.org/abs/2507.17462)|**[link](https://github.com/Xuchen-Li/llm-arxiv-daily)**|
|**2025-07-23**|**Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning**|Joobin Jin et.al.|[2507.17418](https://arxiv.org/abs/2507.17418)|**[link](https://github.com/zezhishao/DailyArXiv)**|
|**2025-07-23**|**Confounded Causal Imitation Learning with Instrumental Variables**|Yan Zeng et.al.|[2507.17309](https://arxiv.org/abs/2507.17309)|null|
|**2025-07-22**|**Pragmatic Policy Development via Interpretable Behavior Cloning**|Anton Matsson et.al.|[2507.17056](https://arxiv.org/abs/2507.17056)|null|
|**2025-07-19**|**Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning**|Yinan Meng et.al.|[2507.16842](https://arxiv.org/abs/2507.16842)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-22**|**GR-3 Technical Report**|Chilam Cheang et.al.|[2507.15493](https://arxiv.org/abs/2507.15493)|null|
|**2025-07-20**|**Reinforcement Learning for Flow-Matching Policies**|Samuel Pfrommer et.al.|[2507.15073](https://arxiv.org/abs/2507.15073)|**[link](https://github.com/iszhanjiawei/flow_matching_arxiv_daily)**|
|**2025-07-20**|**LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading**|Chengwei Lou et.al.|[2507.14995](https://arxiv.org/abs/2507.14995)|**[link](https://github.com/tmgthb/Autonomous-Agents)**|
|**2025-07-18**|**Improving Low-Cost Teleoperation: Augmenting GELLO with Force**|Shivakanth Sujit et.al.|[2507.13602](https://arxiv.org/abs/2507.13602)|null|
|**2025-07-17**|**Latent Policy Steering with Embodiment-Agnostic Pretrained World Models**|Yiqi Wang et.al.|[2507.13340](https://arxiv.org/abs/2507.13340)|**[link](https://github.com/LMD0311/Awesome-World-Model)**|
|**2025-07-17**|**The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner**|Zhouqi Hua et.al.|[2507.13332](https://arxiv.org/abs/2507.13332)|**[link](https://github.com/gabrielchua/daily-ai-papers)**|
|**2025-07-17**|**ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning**|Rahel Rickenbach et.al.|[2507.13088](https://arxiv.org/abs/2507.13088)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-07-17**|**Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)**|Chongli Qin et.al.|[2507.12856](https://arxiv.org/abs/2507.12856)|**[link](https://huggingface.co/models/ChongliQin/iw-SFT-32B)**|
|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al.|[2507.12440](https://arxiv.org/abs/2507.12440)|null|
|**2025-07-15**|**Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning**|Wang Zhicheng et.al.|[2507.10899](https://arxiv.org/abs/2507.10899)|null|
|**2025-07-16**|**GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning**|Ziru Liu et.al.|[2507.10628](https://arxiv.org/abs/2507.10628)|null|
|**2025-07-14**|**Prompt Informed Reinforcement Learning for Visual Coverage Path Planning**|Venkat Margapuri et.al.|[2507.10284](https://arxiv.org/abs/2507.10284)|null|
|**2025-07-14**|**Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?**|Yumi Omori et.al.|[2507.10174](https://arxiv.org/abs/2507.10174)|null|
|**2025-07-13**|**Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles**|Yangang Ren et.al.|[2507.09537](https://arxiv.org/abs/2507.09537)|**[link](https://github.com/Lyz103/LLM-Agent-Paper-daily)**|
|**2025-07-24**|**DAA*: Deep Angular A Star for Image-based Path Planning**|Zhiwei Xu et.al.|[2507.09305](https://arxiv.org/abs/2507.09305)|**[link](https://github.com/52CV/ICCV-2025-Papers)**|
|**2025-07-12**|**Towards Human-level Dexterity via Robot Learning**|Gagan Khandate et.al.|[2507.09117](https://arxiv.org/abs/2507.09117)|null|
|**2025-07-26**|**Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction**|Thomas T. Zhang et.al.|[2507.09061](https://arxiv.org/abs/2507.09061)|null|
|**2025-07-11**|**Behavioral Exploration: Learning to Explore via In-Context Adaptation**|Andrew Wagenmaker et.al.|[2507.09041](https://arxiv.org/abs/2507.09041)|null|
|**2025-07-10**|**Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion**|Lamiaa H. Zain et.al.|[2507.08112](https://arxiv.org/abs/2507.08112)|null|
|**2025-07-15**|**EXPO: Stable Reinforcement Learning with Expressive Policies**|Perry Dong et.al.|[2507.07986](https://arxiv.org/abs/2507.07986)|**[link](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation)**|
|**2025-07-15**|**Reinforcement Learning with Action Chunking**|Qiyang Li et.al.|[2507.07969](https://arxiv.org/abs/2507.07969)|null|
|**2025-07-09**|**Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm**|George Papadopoulos et.al.|[2507.06780](https://arxiv.org/abs/2507.06780)|null|
|**2025-07-13**|**Spatial-Temporal Aware Visuomotor Diffusion Policy Learning**|Zhenyang Liu et.al.|[2507.06710](https://arxiv.org/abs/2507.06710)|**[link](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)**|
|**2025-07-09**|**Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement**|Michael Bloesch et.al.|[2507.06701](https://arxiv.org/abs/2507.06701)|null|

<p align=right>(<a href=#updated-on-20250812>back to top</a>)</p>

## Robotic Navigation

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-08-08**|**Characterization and automated optimization of laser-driven proton beams from converging liquid sheet jet targets**|G. D. Glenn et.al.|[2508.06462](https://arxiv.org/abs/2508.06462)|null|
|**2025-08-08**|**V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles**|Abdullah Zareh Andaryan et.al.|[2508.06404](https://arxiv.org/abs/2508.06404)|null|
|**2025-08-08**|**REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance**|Zihao Xu et.al.|[2508.06229](https://arxiv.org/abs/2508.06229)|null|
|**2025-08-08**|**Depth Jitter: Seeing through the Depth**|Md Sazidur Rahman et.al.|[2508.06227](https://arxiv.org/abs/2508.06227)|null|
|**2025-08-08**|**Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor**|Dominik Brämer et.al.|[2508.06177](https://arxiv.org/abs/2508.06177)|null|
|**2025-08-08**|**GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning**|Xing Lei et.al.|[2508.06108](https://arxiv.org/abs/2508.06108)|null|
|**2025-08-08**|**ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation**|Daniel Lee et.al.|[2508.06065](https://arxiv.org/abs/2508.06065)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-08**|**Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles**|Shaoting Liu et.al.|[2508.05972](https://arxiv.org/abs/2508.05972)|**[link](https://github.com/zezhishao/DailyArXiv)**|
|**2025-08-08**|**It's a Complete Haystack: Understanding Dependency Management Needs in Computer-Aided Design**|Kathy Cheng et.al.|[2508.05940](https://arxiv.org/abs/2508.05940)|null|
|**2025-08-07**|**Sprouting technology otherwise, hospicing negative commons -- Rethinking technology in the transition to sustainability-oriented futures**|Martin Deron et.al.|[2508.05860](https://arxiv.org/abs/2508.05860)|null|
|**2025-08-07**|**Safety of Embodied Navigation: A Survey**|Zixia Wang et.al.|[2508.05855](https://arxiv.org/abs/2508.05855)|null|
|**2025-08-07**|**Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction**|Ahmad Farooq et.al.|[2508.05838](https://arxiv.org/abs/2508.05838)|null|
|**2025-08-07**|**Progress and new challenges in image-based profiling**|Erik Serrano et.al.|[2508.05800](https://arxiv.org/abs/2508.05800)|null|
|**2025-08-07**|**AI-Guided Exploration of Large-Scale Codebases**|Yoseph Berhanu Alebachew et.al.|[2508.05799](https://arxiv.org/abs/2508.05799)|null|
|**2025-08-07**|**GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems**|Keyvan Majd et.al.|[2508.05773](https://arxiv.org/abs/2508.05773)|null|
|**2025-08-07**|**The vast world of quantum advantage**|Hsin-Yuan Huang et.al.|[2508.05720](https://arxiv.org/abs/2508.05720)|null|
|**2025-08-07**|**Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling**|Jianpeng Yao et.al.|[2508.05634](https://arxiv.org/abs/2508.05634)|null|
|**2025-08-07**|**TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution**|Zhikai Zhao et.al.|[2508.05616](https://arxiv.org/abs/2508.05616)|null|
|**2025-08-07**|**CleanUpBench: Embodied Sweeping and Grasping Benchmark**|Wenbo Li et.al.|[2508.05543](https://arxiv.org/abs/2508.05543)|null|
|**2025-08-07**|**Flow-driven magnetic microcatheter for superselective arterial embolization**|Lucio Pancaldi et.al.|[2508.05312](https://arxiv.org/abs/2508.05312)|null|
|**2025-08-07**|**Congestion Mitigation Path Planning for Large-Scale Multi-Agent Navigation in Dense Environments**|Takuro Kato et.al.|[2508.05253](https://arxiv.org/abs/2508.05253)|**[link](https://github.com/jyyang621/DailyArXiv)**|
|**2025-08-07**|**Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models**|Zane Xu et.al.|[2508.05237](https://arxiv.org/abs/2508.05237)|**[link](https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers)**|
|**2025-08-07**|**EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery**|Bingyu Yang et.al.|[2508.05205](https://arxiv.org/abs/2508.05205)|**[link](https://github.com/Ed1sonChen/DailyArxiv)**|
|**2025-08-07**|**Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning**|Wuqiang Zheng et.al.|[2508.05129](https://arxiv.org/abs/2508.05129)|**[link](https://github.com/zachysun/DailyArXiv)**|
|**2025-08-07**|**Cognitive Duality for Adaptive Web Agents**|Jiarun Liu et.al.|[2508.05081](https://arxiv.org/abs/2508.05081)|**[link](https://github.com/BaiShuanghao/my_arXiv_daily)**|
|**2025-08-07**|**MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding**|Weifan Zhang et.al.|[2508.05021](https://arxiv.org/abs/2508.05021)|null|
|**2025-08-07**|**Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots**|Wenjie Hu et.al.|[2508.04994](https://arxiv.org/abs/2508.04994)|null|
|**2025-08-06**|**Engineering Topological Materials**|Amit Goft et.al.|[2508.04927](https://arxiv.org/abs/2508.04927)|**[link](https://github.com/ExpectozJJ/PF-OIHP)**|
|**2025-08-06**|**Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities**|Zixuan Feng et.al.|[2508.04921](https://arxiv.org/abs/2508.04921)|null|
|**2025-08-06**|**Sensitivity of Stability: Theoretical & Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning**|Prabhav Singh et.al.|[2508.04901](https://arxiv.org/abs/2508.04901)|null|
|**2025-08-06**|**At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp**|Yang Liu et.al.|[2508.04821](https://arxiv.org/abs/2508.04821)|null|
|**2025-08-06**|**Open Scene Graphs for Open-World Object-Goal Navigation**|Joel Loo et.al.|[2508.04678](https://arxiv.org/abs/2508.04678)|**[link](https://github.com/ChocoWu/Awesome-Scene-Graph-Generation)**|
|**2025-08-06**|**HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs**|Dengzhao Fang et.al.|[2508.04618](https://arxiv.org/abs/2508.04618)|**[link](https://github.com/KingGugu/DA-CL-4Rec)**|
|**2025-08-06**|**$NavA^3$ : Understanding Any Instruction, Navigating Anywhere, Finding Anything**|Lingfeng Zhang et.al.|[2508.04598](https://arxiv.org/abs/2508.04598)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-06**|**Case Studies of Generative Machine Learning Models for Dynamical Systems**|Nachiket U. Bapat et.al.|[2508.04459](https://arxiv.org/abs/2508.04459)|**[link](https://github.com/CYandYue/Auto-Get-Papers-pro)**|
|**2025-08-06**|**Tactile Comfort: Lowering Heart Rate Through Interactions**|Morten Roed Frederiksen et.al.|[2508.04372](https://arxiv.org/abs/2508.04372)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-07**|**Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research**|Ke Li et.al.|[2508.04326](https://arxiv.org/abs/2508.04326)|**[link](https://github.com/longxiang-ai/awesome-gaussians)**|
|**2025-08-06**|**Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success**|George Bredis et.al.|[2508.04280](https://arxiv.org/abs/2508.04280)|**[link](https://github.com/gabrielchua/daily-ai-papers)**|
|**2025-08-06**|**Unplug, Mute, Avoid: Investigating smart speaker users' privacy protection behaviours in Saudi Homes**|Abdulrhman Alorini et.al.|[2508.04202](https://arxiv.org/abs/2508.04202)|null|
|**2025-08-10**|**DS $^2$ Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation**|Zhaohong Huang et.al.|[2508.04131](https://arxiv.org/abs/2508.04131)|null|
|**2025-08-06**|**Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement**|Chao Hao et.al.|[2508.04025](https://arxiv.org/abs/2508.04025)|null|
|**2025-08-07**|**Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes**|Sanghun Jung et.al.|[2508.03890](https://arxiv.org/abs/2508.03890)|null|
|**2025-08-05**|**When Agents Break Down in Multiagent Path Finding**|Foivos Fioravantes et.al.|[2508.03777](https://arxiv.org/abs/2508.03777)|null|
|**2025-08-05**|**Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways**|Zhongbi Luo et.al.|[2508.03672](https://arxiv.org/abs/2508.03672)|**[link](https://github.com/Zhang-Ziyong/paper_update)**|
|**2025-08-05**|**Personalized Recommendation of Dish and Restaurant Collections on iFood**|Fernando F. Granado et.al.|[2508.03670](https://arxiv.org/abs/2508.03670)|null|
|**2025-08-05**|**Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control**|Hamze Hammami et.al.|[2508.03600](https://arxiv.org/abs/2508.03600)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-05**|**Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions**|Ergi Tushe et.al.|[2508.03541](https://arxiv.org/abs/2508.03541)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-05**|**Investigating a Characteristic Time Lag in the Ionospheric F-region's Response to Solar Flares**|Aisling N. O'Hare et.al.|[2508.03425](https://arxiv.org/abs/2508.03425)|null|
|**2025-08-05**|**VisAug: Facilitating Speech-Rich Web Video Navigation and Engagement with Auto-Generated Visual Augmentations**|Baoquan Zhao et.al.|[2508.03410](https://arxiv.org/abs/2508.03410)|null|
|**2025-08-06**|**Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments**|Ivana Collado-Gonzalez et.al.|[2508.03408](https://arxiv.org/abs/2508.03408)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-05**|**Investigation of Air Fluidization during Intruder Penetration in Sand**|Bowen Wang et.al.|[2508.03350](https://arxiv.org/abs/2508.03350)|**[link](https://github.com/GuanchengWan/awesome-ai-ml-papers-auto)**|
|**2025-08-05**|**Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots**|Zehua Fan et.al.|[2508.03246](https://arxiv.org/abs/2508.03246)|**[link](https://github.com/dingyue772/DailyArxiv)**|
|**2025-08-05**|**Timing is everything: How subtle timing changes in MRI echo planar imaging can significantly alter mechanical vibrations and sound level**|Amir Seginer et.al.|[2508.03220](https://arxiv.org/abs/2508.03220)|null|
|**2025-08-05**|**Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse**|Hikari Yanagawa et.al.|[2508.03216](https://arxiv.org/abs/2508.03216)|null|
|**2025-08-05**|**StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation**|Sangho Suh et.al.|[2508.03182](https://arxiv.org/abs/2508.03182)|null|
|**2025-08-05**|**Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation**|Mintaek Oh et.al.|[2508.03138](https://arxiv.org/abs/2508.03138)|null|
|**2025-08-05**|**Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection**|Le Qiu et.al.|[2508.03129](https://arxiv.org/abs/2508.03129)|null|
|**2025-08-07**|**Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching**|Sirui Chen et.al.|[2508.03068](https://arxiv.org/abs/2508.03068)|null|
|**2025-08-07**|**Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling**|Shuchang Xu et.al.|[2508.03061](https://arxiv.org/abs/2508.03061)|null|
|**2025-08-05**|**SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps**|Haojun Xu et.al.|[2508.03053](https://arxiv.org/abs/2508.03053)|null|
|**2025-08-05**|**CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction**|Yizhuo Wang et.al.|[2508.03027](https://arxiv.org/abs/2508.03027)|**[link](https://github.com/marmotlab/CogniPlan)**|
|**2025-08-06**|**Tool-integrated Reinforcement Learning for Repo Deep Search**|Zexiong Ma et.al.|[2508.03012](https://arxiv.org/abs/2508.03012)|**[link](https://github.com/gabrielchua/daily-ai-papers)**|
|**2025-08-05**|**SustainableQA: A Comprehensive Question Answering Dataset for Corporate Sustainability and EU Taxonomy Reporting**|Mohammed Ali et.al.|[2508.03000](https://arxiv.org/abs/2508.03000)|**[link](https://github.com/DataScienceUIBK/SustainableQA)**|
|**2025-08-05**|**GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring**|Linji Wang et.al.|[2508.02988](https://arxiv.org/abs/2508.02988)|**[link](https://github.com/linjiw/GACL)**|
|**2025-08-05**|**Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling**|Peng Ding et.al.|[2508.02979](https://arxiv.org/abs/2508.02979)|null|
|**2025-08-04**|**Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces**|Vebjørn Haug Kåsene et.al.|[2508.02917](https://arxiv.org/abs/2508.02917)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-04**|**Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators**|Kaixuan Wang et.al.|[2508.02868](https://arxiv.org/abs/2508.02868)|null|
|**2025-08-04**|**Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An Imitation Learning Approach**|Peng Wei et.al.|[2508.02617](https://arxiv.org/abs/2508.02617)|null|
|**2025-08-04**|**PunchPulse: A Physically Demanding Virtual Reality Boxing Game Designed with, for and by Blind and Low-Vision Players**|Sanchita S. Kamath et.al.|[2508.02610](https://arxiv.org/abs/2508.02610)|null|
|**2025-08-04**|**StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes**|Siyi Liu et.al.|[2508.02601](https://arxiv.org/abs/2508.02601)|null|
|**2025-08-04**|**MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming**|Shuo Wang et.al.|[2508.02549](https://arxiv.org/abs/2508.02549)|**[link](https://github.com/luohongk/Awesome-Localization-And-3D-Reconstruction-From-Arxiv)**|
|**2025-08-04**|**Improving Knowledge Graph Understanding with Contextual Views**|Antrea Christou et.al.|[2508.02413](https://arxiv.org/abs/2508.02413)|null|
|**2025-08-04**|**Vision Language Model-based Testing of Industrial Autonomous Mobile Robots**|Jiahui Wu et.al.|[2508.02338](https://arxiv.org/abs/2508.02338)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|
|**2025-08-04**|**Framework for Robust Motion Planning of Tethered Multi-Robot Systems in Marine Environments**|Markus Buchholz et.al.|[2508.02287](https://arxiv.org/abs/2508.02287)|**[link](https://github.com/Ponkux/DailyArXiv-cp)**|
|**2025-08-05**|**After the Party: Navigating the Mapping From Color to Ambient Lighting**|Florin-Alexandru Vasluianu et.al.|[2508.02168](https://arxiv.org/abs/2508.02168)|null|
|**2025-08-07**|**SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents**|Jiaye Lin et.al.|[2508.02085](https://arxiv.org/abs/2508.02085)|null|

<p align=right>(<a href=#updated-on-20250812>back to top</a>)</p>

